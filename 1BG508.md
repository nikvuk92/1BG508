# PGA Course Autumn 2022
# Instructions for project work
The goal of this population genomics project is to familiarize you with the workflows, different types of commonly used analyses as well as a way of thinking about how one approaches questions such as trying to explain the underlying forces that affect populations in general.

The structure of the project work will be organized as follows: 

- each group consists of two students;
- you need an username & password to log into Uppmax;
- up to you to decide if you want to work on your own laptops or in Hubben;
- we have an introductory meeting where we discuss the framework of the exercize in depth;
- in the few weeks you have until the course ends, we will set up meeting times that work for all of us;
- feel free to send in emails with specific questions if something is not working or you're just curious;
- the end product should be a Scientific report of ~10 pages, including Abstract, Introduction, Materials & Methods, Results, Discussion, References, Appendices (length is not crucial here, substance is);
- A short presentation (date will be set);
- the deadline for the written report is the day of the presentation.

# Unmasking the mystery
The specific point of the exercise is for you to identify the populations you have been given in the **'Unknown.fam'**. In order to achieve this, you have been provided a bunch of references (three different datasets) that should come in handy as a way to compare the unknown to the known. Both reference and target populations can be found in the DATA directory:
```
/proj/uppmax2022-2-21/PGA_2022/DATA/Reference_datasets
/proj/uppmax2022-2-21/PGA_2022/DATA/Mystery_populations
```
Helpful scripts are there to aid you on your quest can be found:
```
/proj/uppmax2022-2-21/PGA_2022/SCRIPTS
```
The unknown samples are a mysterious bunch. They look different from the rest don't they? (*hint - check the genotyping rate - i.e. missingness*).
Your first task is to figure out what types of samples you have stumbled upon and why they are (potentially) very valuable, yet they look the way they do (*the real Sherlocks among you would have noticed by now something odd by just glancing at their names!*) 
**Beware** - it is vital to not apply any filters to your unknown samples - we don't know anything about them at this point! 

In the steps that will lead you towards unmasking the unknown populations you will need to: 

1. Check for related individuals in your **reference dataset** 
2. Filter the related individuals from the **reference dataset** - *we have nothing against family, but individuals that are in close kinship will affect our downstream analysis and we don't want that*.
3. Filter the **reference** individuals & SNPs (missingness, HWE, MAF) - *word of caution: do not apply the 'maf' filtering until you have your final references dataset!*
   *Remember: The **unknown** samples **do not** go through any filtering!*
4. Merging the reference datasets (*When merging your reference dataset you want to make sure to only keep the overlapping SNPs*)
6. ????Unify SNP names across your dataset - change the SNP name into the names based on position. This is needed when merging datasets from different SNP chips (the same position can have different names on different chips). 
7. After merging together your reference dataset and you've finished filtering it - merge it together with your **unknown** data.
8. ???Before running PCA you need to prune your data for SNPs in LD (pruning is explained in the admixture manual https://dalexander.github.io/admixture/admixture-manual.pdf)
9. Run a PCA onjust on your "reference" dataset as well as on the combined "reference + Unknown dataset".
10. Run a projected PCA - but why?
11. Run Admixture (remember - Admixture takes a lot of time! Organize your time wisely)
12. Read up on some literature - it will help for writing the report! Pay special attention to the key figures in them.
[Schlebusch 2012](https://pubmed.ncbi.nlm.nih.gov/22997136/)
[Gurdasani_2015](https://www.nature.com/articles/nature13997) 
[Patin 2017](https://science.sciencemag.org/content/356/6337/543)
[Vicente_2021](https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-021-01193-z)
13. Don't panic - there's plenty of time. Or is there?

### Some technicalities

* You should have R studio installed on your computer

Before we start here are some basic Unix/Linux commands if you are not used to working in a Unix-style terminal:

## Using the batch system

# Logging into SNOWY:

```
ssh -AX YOUR_USERNAME@snowy.uppmax.uu.se
```
After which you type in your password. *And yes, it'll not show up when typing it, but if you type it in correctly & press enter, you're in.*

Note: When acessing SNOWY from Rackhams login nodes you must always use the flag -M for all SLURM commands.
Examples:

```
- squeue -M snowy
- jobinfo -M snowy
- sbatch -M snowy slurm_script_file
- scancel -u username -M snowy
- interactive -A projectname -M snowy -p node -n 32 -t 01:00:00

```

Note: It is recommended to load all your modules in your job script file. This is even more important when running on Snowy since the module environment is not the same on the Rackham login nodes as on Snowy compute nodes.
You can read up more on how to use SNOWY : [Snowy_User_guide](https://www.uppmax.uu.se/support/user-guides/snowy-user-guide/)

Generally, for any bigger job (bigger Plink jobs, PCA, Admixture) always work in interactive mode.

### How to run interactively on a compute node?:

```
   interactive -A uppmax2022-2-21 -M snowy -p node -n 32 -t 02:00:00
```
### How to submit jobs on SNOWY?
```

```
### Moving about:
```
    cd – change directory
    pwd – display the name of your current directory
    ls – list names of files in a directory
```
### File/Directory manipulations:
```
    cp – copy a file
    mv – move or rename files or directories
    mkdir – make a directory
    rm – remove files or directories
    rmdir – remove a directory
```
### Display file content:
```
    cat - concatenate file contents to the screen or to a file
    less - open file for viewing
    more 
    tail
    head
    realpath - checking the path to a file 
```	
When creating and editing a file in text editor you can use ```nano``` or ```vim``` or something else.

### Analysis steps

## PART 1 Filtering and merging population genomic data using PLINK  	    

FYI: Link to PLINK site:[https://www.cog-genomics.org/plink2](https://www.cog-genomics.org/plink2)

PLINK is a software for fast and efficient filtering, merging, editing of large SNP datasets, and exports to different outputs directly usable in other programs. Stores huge datasets in compact binary format from which it reads-in data very quickly.

Running the program:

If working on Rackham type in the following:
```
module load bioinfo-tools
module load plink/1.90b4.9 

```
The software is already pre-installed on Rackham, you just have to load it

Try it:
```
plink
```

#### This is what a basic command structure looks like in Plink: 
``` 
plink --filetypeflag filename --commandflag commandspecification --outputfilecommand --out outputfilename
```
For example to do filtering of missing markers at 10% frequency cutoff (reading-in a bed format file called file1, doing the filtering, writing to a ped format file called file2):
```
plink --bfile file1 --geno 0.1 --recode --out file2
```

#### Input formats:
File format summary:
ped format: usual format (.ped and .fam)
.ped contain marker and genotype info and .fam files contain sample info
bed format: binary/compact ped format (.fam .bim and .bed)
(.fam - sample info  .bim - marker info  and .bed - genotype info in binary format
tped format: transposed ped format (.tfam and .tped files)
tfam sample info, .tped marker and genotype info in transposed format
lgen format: long format (see manual, not used that often)

# Setup
Navigate to the directory you want to work in.
```
cd path (change directory)
mkdir directory_name (create a new directory)
```
If you don't already have a working directory for this course then create one now:

```
cd /proj/uppmax2022-2-21 #Uppmax project for this course
mkdir your_unique_team_name
```

## Getting your project datasets. Reading in bed file and converting to other formats 


#### The course material can be found at the following path:

```
/proj/uppmax2022-2-21/PGA_2022
```

Copy the datasets from the directory “DATA” to your working folder by **changing** the command below while you are in your working folder. The **dot** means copy the contents "here":
```
cp /full_path_to_course_material/unk1.bed .
cp /full_path_to_course_material/unk1.bim .
cp /full_path_to_course_material/unk1.fam .
```

# These files contain SNPs from 16 unknown individuals in bed file format. You are going to figure out the ancestry of these population groups during this practical.

Look at the `.bim` (markers) and `.fam` (sample info) files by typing:

```
less Unknown.bim
``` 

do the same for the `.fam` file

```
less Unknown.fam 
```
As mentioned before the `.bim` file store the variant markers present in the data and `.fam` lists the samples. (you can try to look at the .bed as well, but this file is in binary format and cannot be visualized as text if you want to see the specific genotype info you must export the bed format to a ped format)

Read in a  bed file dataset and convert to ped format by typing/pasting in:

```
plink --bfile Unknown --recode --out Unknown_ped 
```

#### Look at the info that Plink prints to the screen. How many SNPs are there in the data? How many individuals? How much missing data?

Look at the missingness information of each individual and SNP by typing:

```
plink  --bfile Unknown --missing --out test1miss
```

Look at the two generated files by using the `less` command (q to quit)

```
less test1miss.imiss
less test1miss.lmiss
```

The `.imiss` contains the individual missingness and the `.lmiss` the marker missingness
Do you understand the columns of the files? The last three columns are the number of missing, the total, and the fraction of missing markers and individuals for the two files respectively

## Look at the first few lines of the newly generated .map (sample info) and .ped (marker and genotype info) files using the more command as demonstrated above

Read in bed/ped file and convert to tped

```
plink --bfile Unknown --recode transpose --out Unknown_tped 
plink --file Unknown_ped --recode transpose --out Unknown_tped 
```

Do you see the difference in the two commands above for reading from a bed (--bfile) and reading from a ped (--file) file. Which one takes longer to read-in?

Look at the first few lines of the  `.tfam` and `.tped` files by using the `less` command

## Can you see what symbol is used to encode missing data?

Note- try to always work with bed files, they are much smaller and takes less time to read in. 
See this for yourself by inspecting the difference in file sizes:

```
ls -lh * 
```

Plink can convert to other file formats as well, you can have a look in the manual for the different types of conversions

### Data Merging

The next step would be to start merging your data with comparative datasets. But first you need to merge the three reference datasets to eachother, filter and clean-up, before you finally merge the **combined** to the **Unknown** dataset. 

Usually, when you merge your data with another dataset there are strand issues. The SNPs in the other dataset might be typed on the reverse DNA strand and yours on the forward, or vice versa. Therefore you need to flip the strand to the other orientation for all the SNPs where there is a strand mismatch. One should not flip C/G and A/T SNPs because one cannot distinguish reverse and forward orientation (i.e. C/G becomes G/C unlike other SNPs i.e. G/T which become C/A). Therefore before merging and flipping all A/T and C/G SNPs must be excluded. However, this can be a problem since some of your SNPs in your dataset may be monomorphic when you don't apply the MAF filter. I.E in the bim file they will appear as C 0 (with 0 meaning missing). So you don't know what kind of SNP it is, it can be C G or C T for instance if it is C G it needs to be filtered out but not if it is C T.
Therefore, before merging our data to other datasets it is important to first merge your data with a fake / reference_individual, that you prepare, which is heterozygous at every SNP position. This “fake” reference individual you can easily prepare from the SNP info file you get from the genotyping company or your own genomics processing software (such as Genome Studio from Illumina). You can also prepare it from data downloaded for each SNP from a web-database such as dbSNP. 

# The proper way of doing this takes might take a bit more time and involves more steps but it will yield a higher genotyping rate. If you feel like you have plenty of time on your hands, feel free to follow the OPTIONAL 2 - Data Merging and strand flipping(https://github.com/Hammarn/Populationsgenomik/blob/master/1BG508.md) chapter. If you decide to do this use the datasets in the directory named OPTIONAL.

# Due to time constraints, we are giving you a more simplified albeit cruder way to do this. If done correctly it should take us to a similar end goal, which is what we ultimately need for the PCA/Admixture analyses.

*P.S. You can also check out the Plink(https://www.cog-genomics.org/plink/1.9/data) documentation on how to do this. 


## Merging and strand flipping

We start by merging one dataset (Ex.Schlebusch_2012) to another one (ex.Patin_2017):
**P.S. As long as you have all the files/datasets in the same directory you don't need to specify the full paths. If they are in different datasets, remember to add the full path to the file, such as ```/proj/uppmax2022-2-21/PGA_2022/full_path_to_YOUR_OWN_directory/Reference_datasets/datasetA/file```**

**P.P.S. Do this in an interactive node!

``` 
plink --bfile Schlebusch_2012 --bmerge Patin_2017 --recode transpose --allow-no-sex --out Merged_Patin_Schlebusch
```

This, expectedly produces an error, because of the strand mismatches:
```
Error: 49 variants with 3+ alleles present.
```
What you need to do then is to flip those positions using the ```.missnp``` file that is produced after the error:

```
plink --bfile Patin_2017 --flip Merged_Patin_Schlebusch.missnp --make-bed --out Patin_corrected
```
After this, if you try again, you should be able to do the merge :

```
plink --bfile Schlebusch_2012 --bmerge Patin_corrected --recode transpose --allow-no-sex --out Merged_Patin_Schlebusch
```
Apply the same logic when merging the third dataset to the merged dataset of the first two. Once again, you will have strand issues, so try flipping again, and remerging. As long as you didn't lose track of what you're merging to what, everything should be working as expected.

When you're done with merging the three reference datasets to eachother, and you have them in one file, continue with the filtering steps below.

### Filtering for missing data:

First, we filter for marker missingness:

Paste in the command below to filter out markers with more than 10% missing data

```
plink --bfile combined_dataset --geno 0.1 --make-bed --out combined_dataset2 
```

Look at the screen output, how many SNPs were excluded?

### Now we will filter for individual missingness:

Paste in the command below to filter out individual missingness

```
plink --bfile combined_dataset2 --mind 0.15 --make-bed --out combined_dataset3 
```

Look at the screen output, how many individuals were excluded?

### MAF filtering:
To filter for minimum allele frequency is not always optimal, especially if you are going to merge your data with other datasets in which the alleles might be present. But we will apply a MAF filter in this case
(remember not to filter the 'Unknown' samples)

Filter data for a minimum allele frequency of 1% by pasting in:

```
plink --bfile combined_dataset3 --maf 0.01 --make-bed --out combined_dataset4 
```

How many SNPs are left at this point?

### Now we will filter for SNPs out of Hardy-Weinberg equilibrium. 
Most likely, SNPs out of HWE usually indicates problems with the genotyping. However, to avoid filtering out SNPs that are selected for/against in certain groups (especially when working with case/control data) filtering HWE per group is recommended. After, only exclude the common SNPs that fall out of the HWE in the different groups - (OPTIONAL). But for reasons of time, we will now just filter the entire dataset for SNPs that aren’t in HWE with a significance value of 0.001

```
plink --bfile combined_dataset4 --hwe 0.001 --make-bed --out combined_dataset5
```

Look at the screen. How many SNPs were excluded?

If you only what to look at the HWE stats you can do as follows. By doing this command you can also obtain the observed and expected heterozygosities. 

```
plink --bfile combined_dataset5 --hardy --out hardy_combined_dataset5
```
Look at file hardy_combined_dataset5.hwe, see if you understand the output?

There are additional filtering steps that you can go further. PLINK site on the side lists all the cool commands that you can use to treat your data. Usually, we also filter for related individuals and do a sex-check on the X-chromosome to check for sample mix-ups. 

### Filtering out related individuals - to be reviewed:

In the `SCRIPTS` folder, there is a script called `sbatch_KING.sh` that can be used to run [KING](http://people.virginia.edu/~wc9c/KING/manual.html) You can have a look inside for instructions on how to run the script. Check out the manual and try and figure out how the software works.
After you have run the script have a look at the produced output files and figure out how to remove the related individuals. 
*P.S. You don't need to do run KING for the Unknown dataset.*

The script for KING is as follows:
```
#!/bin/bash -l
#
#
#SBATCH -J king
#SBATCH -t 12:00:00
#SBATCH -A uppmax2022-2-21
#SBATCH -n 8
## USAGE: sbatch this_script.sh bedfile
king -b $1  --unrelated
```
You can submit it as a job by doing:

```
sbatch -M snowy sbatch_KING.sh A_DATASET.bim
```
You can check your job:

```
jobinfo -u YOUR_USERNAME -M snowy
```

### After you are done with all the filtering steps you can finally merge the filtered combined reference dataset with the Unknown.

## When merging datasets from different chips, the same position can have different names on different chips. The ```rename_SNP.py``` script can be used on the `.bim` files to change the SNP name into the names based on position. Luckily for you, this has already been done for the three reference datasets. Otherwise there would have been just the additional step of renaming the SNPs for each dataset prior to merging.
But since we don't know where the Unknown dataset came from we want to conduct this step for it. Check the bim file for the Unknown dataset and see the SNP names before you rename them.  

In order to run ```rename_SNP.py``` first load ```module load python/2.7.11``` (Issues might occur if not using the right version of Python).

```
python rename_SNP.py Unknown.bim
```
After which you should get that the output with replaced SNP names has been written to ```Unknown.bim_pos_names```.
You can delete the original bim (or rename it if you're feeling unsure) and edit the ```Unknown.bim_pos_names``` to be ```Unknown.bim```.
Check to see what happened after the script did its job.

### Merging the merged reference dataset to the Unknown

At this point you should have merged all three reference datasets to each other, conducted all the filtering & QC steps and renamed the SNP names in the Unknown. Now merge the reference dataset to the Unknown following the same logic as above. 

```
plink --bfile COMBINED_DATASET --bmerge UNKNOWN.bed UNKNOWN.bim UNKNOWN.fam --make-bed --out FINAL_MERGE  
```
There will be *at least* one strand flipping issue. 
Try and see if after a few flips you can merge the two datasets together.
If this is still not working, it is possible that a few positions are causing this issue. In this case (assuming its just a few - check what the output says) you can exclude those problematic variants and remerge :

```
plink --bfile source1 --exclude merged.missnp --make-bed --out source1_tmp
plink --bfile source2 --exclude merged.missnp --make-bed --out source2_tmp
plink --bfile source1_tmp --bmerge source2_tmp --recode transpose --allow-no-sex --out PopStrucIn1
rm source1_tmp.*
rm source2_tmp.*
```
Make sure to remember what your two final datasets are called. The one where you merged all the reference datasets and the one where you did that but also added the Unknown, which we are calling ```PopStrucIn1``` from now on. In the next steps you will need to produce a PCA of both.

How many SNPs are left for your analyses? What was the genotyping rate for the combined reference dataset and what was it for the unknowns? Finally what is the genotyping rate for the final merged dataset? 

This are the final files for the next exercise. 
Now you have generated your input files for the next exercise which will deal with population structure analysis. You will look at the population structure of your unknown samples in comparison to the known reference populations that came from HapMap and HGDP.

=============================================================================

## PART 2: Population structure inference 

# Word of advice - the following analyses might take quite some time to run. Therefore, plan accordingly and leave yourself enough room for error.
Furthermore, make sure to doublecheck your script before running it. 90% of issues you will encounter when running scripts are going to be typos (Sanchez,R., Smith,M., 2022). 
Check if you have written the correct file names, the correct extensions, whether you have the right path to your file or whether you have typed in your correct project name. Since we have a limited CPU hours allocated for this course it is really important to be efficient!  

## ADMIXTURE 

Before running ADMIXTURE it is advisable to prune the data to thin the marker set for linkage disequilibrium.
You can read up on how to prune for LD(https://dalexander.github.io/admixture/admixture-manual.pdf).

```plink --bfile YOUR_DATASET --indep-pairwise 10 10 0.1
   plink --bfile YOUR_DATASET --extract plink.prune.in --make-bed --out prunedData
```

Using ADMIXTURE/PONG and principal component analysis with EIGENSOFT 

ADMIXTURE is a similar tool to STRUCTURE but runs much quicker, especially on large datasets.
ADMIXTURE runs directly from .bed or .ped files and needs no extra parameters pr file preparation. You do not specify burin and repeats, ADMIXTURE exits when it converged on a solution (Delta< minimum value)


First, you have to load the module:

```
module load bioinfo-tools
module load ADMIXTURE/1.3.0

```

A basic ADMIXTURE run looks like this:

```
admixture -s time PopStrucIn1.bed 2
```

This command executes the program with a seed set from system clock time, it gives the input file (remember the extension) and the K value at which to run ADMIXTURE (2 in the previous command).

For ADMIXTURE you also need to run many iterations at each K value, thus a compute cluster and some scripting is useful.

Make a script from the code below to run Admixture for K = 2-6 with 3 iterations at each K value.

```
#!/bin/bash
for kval in {2..6}; do
   for itir in {1..3}; do
           (echo '#!/bin/bash -l'
           echo "
           mkdir ${kval}.${itir}
           cd ${kval}.${itir}
#working folder where admixture and the bed files are
module load bioinfo-tools
module load ADMIXTURE/1.3.0
admixture -j3 -s $RANDOM path/to/your/bedfile.bed ${kval}
cd ../
rm -r ${kval}.${int}
#moves it to a different folder and renames it so you end up with multiple iterations
exit 0") |
               sbatch -M snowy -p core -n 3 -t 72:0:0 -A your_uppmax_project_here -J ADMX.${kval}.${itir} -o ADMX.${kval}.${itir}.output -e ADMX${kval}.${itir}.output
       done
done
```

You will need to **replace the project name** and the **path** to your pruned .bed-file in the script then just run it:

```
bash NAME_OF_THE_SCRIPT.sh
```

## PONG 

When the admixture analysis is finally done we use PONG to combine the different iterations and visualize our results. P

A typical PONG command looks like this:


```
pong -m your_filemap.txt -i your_ind2pop.txt -n your_pop_order.txt -g
```

To be able to run PONG we thus need to generate three different files.

The first being the ***filemap***. This is the only input that is strictly required to run PONG. It consists of three columns.
From the PONG manual: 


```
Column 1. The runID, a unique label for the Q matrix (e.g. the string “run5_K7”).

Column 2. The K value for the Q matrix. Each value of K between Kmin and Kmax must
be represented by at least one Q matrix in the filemap; if not, pong will abort.

Column 3. The path to the Q matrix, relative to the location of the filemap. 
```

In order to create what we need we can edit and run the loop below.
Make sure whether the prefix of the admixture output Q files is the same in both the loop and the Q files. As long as you are starting PONG in the same directory of all the Admixture output files you should be ok.


```
#!/bin/sh
module load bioinfo-tools

for k in {2..5};
do
    for j in {1..10};
    do
    echo -e "k${k}_r${j}\t$k\t${k}.${j}/ADMX.${k}.Q" >> Unknown_FILEMAP.txt
    done
done

```

The next file we need to create is the ***ind2pop*** file. It is just a list of which population each individual belongs to.
We have this information in the `.fam` file so we can just cut out the field we need:

```
cut -f 1 -d " " PopStrucIn1.fam > unknown_IND2POP.txt
``` 
Example of how it looks like

```
Neanderthal001
Denisovan1990
Otzi006
Cheddar_man001
Bigfoot_North.SDG
Rick_or_Morty
King_Arthur_ENG.SDG
Russian.SDG
English.SDG
Spanish.HG
Bantu.RHG

```

The ***poporder*** file is a key between what your populations are called and what "Proper" name you want to show up in your final plot.
For us, it will look like this. *Note that the file needs to be **tab-delimited**, i.e separated by tabs (If you have spaces issues may occure). Also, make sure within the names in this file to not have any spaces. If you do, try replacing them with underscores (_).*
The poporder file, as the name suggests, **gives the order in which you want the populations to be in**, so order them as you think it makes most sense.
*Again, be careful when editing this file, typos, extra spaces etc might cause problems.*

Below is just an example of what a poporder file should look like

```
aDNA_Rick	pickle
aDNA_Morty	m0rty
aDNA_Ragnar	rgnr
aDNA_martians	mrt99
Napoleon_Bonaparte	nb001
Netherlands_BA	Dutch

```
So just copy this into a file called `unknown_POPORDER.txt`

## Now we have all the files we need. Time to run PONG.
*Pong is most easily ran locally on your computer. But if you don't know how to install it is available through the module system on Uppmax. 

```
module load pong 
```

Since we are several people who are going to run PONG at the same time we need to use a different port, otherwise, we will collide with each other. The default port for PONG is 4000. Any other free port will work, like 4001, 2, etc. Make sure you are using a **unique port** before proceeding. If multiple people are trying to run PONG on the same port you have problems, so talk to your classmates and find a unique port in the 4000s that people aren't using.

```
pong -m unknown_FILEMAP.txt -i unknown_IND2POP.txt -n unknown_POPORDER.txt -g --port YOUR_PORT_NUMBER_HERE
```


When PONG is done it will start hosting a webserver that displays the results at port 4000 by default:  http://localhost:4000. Pong needs to be running for you to look at the results, i.e. **if you close it it will not work.**

The web server is hosted on the same login node as you were running pong. In case you are unsure of which one that is you can use the command `hostname` to figure it out:

```
hostname
```

To view files interactively you need to have an X11 connection. So when you connect to rackham do:


```
ssh -AY YOUR_USERNAME_HERE@rackham.uppmax.uu.se

```

Make sure that you connect to **the same Rackham (i.e 1,2,3 etc) as you got from hostname**.  
In a **new tab** (if you didn't put PONG in the background) type:

```
firefox http://localhost:YOUR_PORT_NUMBER
```


#### What do you see? What information does this give you about your misterious unknown populations? 
#### Can you figure out who they are?

Once you are finished with looking at the PONG output you can click and save some output to file and then close the program by hitting `ctrl - c` in the terminal tab running it. 


### Principal component Analysis with Eigensoft

The next population structure method we will look at is Principal Components Analysis with Eigensoft. 

You run eigensoft on the .bed format from plink. You only need to modify your .fam file a little for it to work in Eigensoft. The .bed and .map files you use directly as-is. The .fam file you change the extension to .pedind and you substitute the last column (-9 at the moment indicating missing phenotype) with population numbers. When assigning pop numbers do not use 1, 2 or 9. They are reserved for cases, controls and missing data in Eigensoft.

Paste this piece of code in to the terminal: <----------fix the code below

```
cut -d " " -f1-5 PopStrucIn1.fam >file1a
cut -d " " -f1 PopStrucIn1.fam >file2a
sed "s/Unknown1/51/g" <file2a | sed "s/Unknown3/53/g" | sed "s/Unknown5/55/g" | sed "s/Unknown11/61/g" | sed "s/CEU/81/g" | sed "s/YRI/82/g" | sed "s/Han/83/g" | sed "s/San/84/g" | sed "s/MbutiPygmies/85/g" >file3a
paste file1a file3a >fileComb
sed "s/\t/ /g" fileComb > PopStrucIn1.pedind
rm file1a; rm file2a; rm file3a; rm fileComb
```
It will make a `.pedind` file from your `.fam` file.

Furthermore, you need a parameter file to indicate your parameter options to EIGENSOFT.

Copy the prepared parameter file from the `DATA` directory to your working folder it's called 
`PopStrucIn1.par`

Open the parameter file and look at what is specified in it. At the start is the input/output files. Furthermore, we ask for the info for 10 PCs to be output, qtmode is set to NO to indicate more than one pop, we prune the SNPs based on LD for an r2 value of 0.2. We don't remove any outlying points and we limit the sample size to 20. This is important for PCA where there are groups with very large sample sizes since large sample sizes will distort the PC plot. It is best for PCA that sample sizes are as even as possible.

Run the smartpca package in Eigensoft by typing

```
module load eigensoft

smartpca -p PopStrucIn1.par
```

The outputfiles are .evec and .eval

In the `.evec` file is the main output for the number of PCs that you specified in the .par file. The first row is the Eigenvalues for each of your PCs the rest of the rows list your Pop:Ind specification, the PCs and their loadings and your PopNumber at the end. in the `.eval` is all the eigenvalues that were extracted. To work out the percentage of variation each PC explains, you divide your particular PC eigenvalue by the sum over all the eigenvalues.

We will plot the PCs in R now


Prep for R:

```
sed 1d PopStrucIn1.evec | sed  "s/:/   /g " >   PopStrucIn1.evecm
```


Open `R` and paste the following code to plot your PCs:

```
WD<-getwd()
setwd(WD)
## Define these
evec<- read.table ("PopStrucIn1.evecm")
eval<- read.table ("PopStrucIn1.eval")
namer <- "PopStrucIn1"
nrpc<-10
## Script start
totalev <-sum(eval)
aa <- array(NA,dim=c(nrpc,1))
for (i in 1:nrpc) {
    aa[i,1]<-format(round(((eval[i,1]/totalev)*100),3), nsmall = 3)}
pdf (file =paste(namer, "_PCA1.pdf", sep=""), width =10, height = 15, pointsize =12)
par(mfrow=c(3,2), oma=c(0,0,4,0))
plot (evec[,3], evec[,4],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC1: ", aa[1,1], "%", sep=""), ylab=paste("PC2: ", aa[2,1], "%", sep=""))
legend("topright",  legend = unique(evec[,1]), text.col = "black", cex = 0.75, pch =unique(evec[,1]), col = unique(evec[,1]), xpd = TRUE, bty="n", )
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
plot (evec[,5], evec[,6],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC3: ", aa[3,1], "%", sep=""), ylab=paste("PC4: ", aa[4,1], "%", sep=""))
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
plot (evec[,7], evec[,8],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC5: ", aa[5,1], "%", sep=""), ylab=paste("PC6: ", aa[6,1], "%", sep=""))
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
plot (evec[,9], evec[,10],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC7: ", aa[7,1], "%", sep=""), ylab=paste("PC8: ", aa[8,1], "%", sep=""))
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
plot (evec[,11], evec[,12],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC9: ", aa[9,1], "%", sep=""), ylab=paste("PC10: ", aa[10,1], "%", sep=""))
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
barplot (as.numeric(aa[,1]), xlab = "PC", ylab = "%Variation explained", axes=TRUE)
title(paste(namer, "PC plot"), outer=TRUE, cex.main = 1)
dev.off()
q()
```

See if you understand the code above. It plots PC1vsPC2 etc and puts labels on the plot.

Look at the output PDF. Do the results of your population PCA correspond to the population structure results you got from the ADMIXTURE plots? How many of the PCs do you think contain useful information. What part of the variation is represented by each of the PCs. Can you see the percentage variation that each PC explains?
It is a good idea to make two PCA plots - one with just the combined reference dataset and another one with the combined reference dataset + the Unknowns and compare them.



### Running a projected PCA

For our final analysis of the project - the projected PCA, you first need to produce some files:
  - a tped file (your final merged dataset); 
  - a list of the modern populations in the dataset;
  - a list of the unknown populations in the dataset;

To produce the tped, you can use plink as you've done so far. 

And to produce the two population lists:

```
awk {'print $1'} YOUR_DATASET.tfam |sort|uniq > Sorted_list_of_the_populations.txt
```
Then, this sorted list of populations, you can subset to an ```unknown.txt``` and a ```modern.txt``` using any approach you like.
*Note* Populations IDs in <modern reference pops> must exactly match the IDs in the tfam, while population IDs in <unkown pops to include> can be more general (e.g. HG_SE instead of HG_SE_SF12) -- No error messages when this is violated but weird results may occur!

After you've sorted the abovementioned you can go ahead & submit a sbatch job.
When submitting an job on SNOWY you need to create a sh file first. You can use ```nano``` or any other text editor for creating that file:

Ex.
``` 
#!/bin/bash -l
#SBATCH -J Projected
#SBATCH -t 2-00:00:00
#SBATCH -A uppmax2022-2-21
#SBATCH -n  4
bash pca_lsqproj.sh YOUR_DATASET_IN_TPED_FORM modern.txt unknown.txt
``` 

This should take a few hours. You can check whether the job is still running with ``` sacct ``` or ```jobinfo```.

When it's done, assuming it has ended sucessfuly, copy the ```.evec`` and ```.eval``` files to your computer's R directory and plot the PCA.
There are multiple ways of doing this, but if you can't think of a better one here's a suggestion:

```
library(ggplot2)

##################### LSQ PROJECTION PCA PLOT #######################
pca<-read.table("merged.popsubset.evec")
####### DIVIDE THE TABLE INTO MODERN & UNKNOWN - make sure to edit the right numbers! ########

modern<-pca[c(1:1610),]
modern$V12<-factor(modern$V12)
ancient<-pca[c(1611:1626),]

### PLOTTING THE MODERN SAMPLES 
par(mfrow=c(1,1))
layout(matrix(c(1,1), ncol=1), heights=c(1.5, 1))
par(mar=c(4,4,1,1))
with(modern,plot(modern$V2~modern$V3,pch=20,cex=0.5,col = rgb(0, 0, 0, 0.15),xlab="PCA2",ylab="PCA1"))

### LABELING THE PLOT CREATED WITH THE MODERN SAMPLES 

mod_lab2<-aggregate(modern[,2:3],list(modern$V12),mean)
for(j in 1:100) 
{
  text(mod_lab2$V3[j],mod_lab2$V2[j],labels=mod_lab2$Group.1[j],cex=0.7)   
}

### SPECIFYING THE UNKOWN SAMPLES - change the numbers accordingly based on the position in the final file!

UNK1<-pca[c(1611),]
UNK2<-pca[c(1612),]
UNK3<-pca[c(1613),]
UNK4<-pca[c(1614),]
UNK5<-pca[c(1615),]
UNK6<-pca[c(1616),]
UNK7<-pca[c(1617),]
UNK8<-pca[c(1618),]
UNK9<-pca[c(1619),]
UNK10<-pca[c(1620),]
UNK11<-pca[c(1621),]
UNK12<-pca[c(1622),]
UNK13<-pca[c(1623),]
UNK14<-pca[c(1624),]
UNK15<-pca[c(1625),]
UNK16<-pca[c(1626),]


### PLOTTING THE UNKNOWN SAMPLES 

UNK1$V12<-factor(UNK1$V12)
points(UNK1$V3,UNK1$V2,pch=22,cex=1.4,col="burlywood",bg="burlywood")
for(k in 1:35) 
  
{
  text(UNK1$V3[k],UNK1$V2[k],labels=UNK1$V12[k],pos=c(4),cex=0.9)   
}

UNK2$V12<-factor(UNK2$V12)
points(UNK2$V3,UNK2$V2,pch=17,cex=1.4,col="yellow3",bg="yellow3")
for(k in 1:13) 
  
{
  text(UNK2$V3[k],UNK2$V2[k],labels=UNK2$V12[k],pos=c(3),cex=0.9)   
}

UNK3$V12<-factor(UNK3$V12)
points(UNK3$V3,UNK3$V2,pch=15,cex=1.4,col="yellow2",bg="yellow2")
for(k in 1:13) 
  
{
  text(UNK3$V3[k],UNK3$V2[k],labels=UNK3$V12[k],pos=c(3),cex=0.9)   
}

UNK4$V12<-factor(UNK4$V12)
points(UNK4$V3,UNK4$V2,pch=25,cex=1.4,col="palevioletred2",bg="palevioletred2")
for(k in 1:13) 
  
{
  text(UNK4$V3[k],UNK4$V2[k],labels=UNK4$V12[k],pos=c(1),cex=0.9)   
}

```
Most likely you can guess where this is going, so just use this as a reference and write the R script for the rest of your unknown individuals.
After running it, you should get your nice projected PCA. Look at the output PDF, what has changed? This should give you a further indication as to who your Unknown populations are.


