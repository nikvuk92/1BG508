# Populationgenomik 1BG508 [2023 Version]

## Instructions for project work

The goal of this population genomics project is to familiarize you with the workflows, different types of commonly used analyses as well as a way of thinking about how one approaches questions such as trying to explain the underlying forces that affect populations in general. We know that people tend to migrate and throughout history they have done that a lot. Similar populations get isolated and with time may become more distant, distant peoples admix and over time may become more closely related - therefore trying to decipher human population history is usually not an easy task.

A few technical details first:

- each student works on the exercize on their own;
- There is an UPPMAX project and its name is UPPMAX 2023/2-1 - you will be working here;
- you need an username & password to log into Uppmax (you already have these by now);
- You will be working on the local computers at campus;
- If by any chance you decide to work on your own computer, make sure to have all the necessary programs installed beforehand.

## Unmasking the mystery of the four populations

The specific point of the exercise is for you to identify the populations you have been given in the **'unk1.fam'**. In order to achieve this, you have been provided a bunch of references that should come in handy as a way to compare the unknown to the known. Both reference and target populations can be found in the directory:
```
/proj/uppmax2023-2-1/MYSTERY_QUEST/  
```
The exercize is divided into two sub-sections.
The first part consists of different filtering steps that you need to conduct on the data that you have been provided.
After you've completed that, the next thing to do is to merge the datasets together.
The second part is the analysis of the data you have been working with & filtering. This includes analysing the PCA & Admixture results. 
Finally, you will make inferences and try to explain what you see.

It is good to read up on some literature:
[Schlebusch 2012](https://pubmed.ncbi.nlm.nih.gov/22997136/)
[Gurdasani_2015](https://www.nature.com/articles/nature13997) 
[Patin 2017](https://science.sciencemag.org/content/356/6337/543)
[Vicente_2021](https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-021-01193-z)

## Brief overview of working in a Unix-style terminal:

### Logging into SNOWY & working on the server:
```
ssh -AX user@rackham.uppmax.uu.se 
```
After which you type in your password. *And yes, it'll not show up when typing it, but if you type it in correctly & press enter, you're in.*

Note: When acessing SNOWY from Rackham's login nodes you must always use the flag ```-M``` for all SLURM commands.
 
Some examples:

 ```
 - squeue -M snowy
 - jobinfo -M snowy
 - sbatch -M snowy slurm_script_file
 - scancel -u username -M snowy
 - interactive -A projectname -M snowy -p node -n 32 -t 01:00:00
 ```

Note: It is recommended to load all your modules in your job script file. This is even more important when running on Snowy since the module environment is not the same on the Rackham login nodes as on Snowy compute nodes.
You can read up more on how to use SNOWY : [Snowy_User_guide](https://www.uppmax.uu.se/support/user-guides/snowy-user-guide/)

Generally, for any bigger job (bigger Plink jobs, PCA, Admixture) **always work in interactive mode**.

 ### How to run interactively on a compute node:
Ex.:
 ```
    interactive -A uppmax2023-2-1 -M snowy -p node -n 32 -t 02:00:00
 ```
### Moving about:
```
    cd – change directory
    pwd – display the name of your current directory
    ls – list names of files in a directory
```
### File/Directory manipulations:
```
    cp – copy a file
    mv – move or rename files or directories
    mkdir – make a directory
    rm – remove files or directories
    rmdir – remove a directory
```
### Display file content:
```
    cat - concatenate file contents to the screen/file
    less - open file for viewing
    more 
    tail
    head
    realpath - checking the path to a file 
```
When creating and editing a file in text editor you can use ```nano``` or ```vim``` or something else.

### You can check the status of your jobs with:

 ```
 jobinfo -u YOUR_USERNAME -M snowy
 ```

# PART 0 Knowing your way around & using Plink   	    

PLINK is a software for fast and efficient filtering, merging, editing of large SNP datasets, and exports to different outputs directly usable in other programs. Stores huge datasets in compact binary format from which it reads-in data very quickly.
FYI: Link to PLINK site:[https://www.cog-genomics.org/plink2](https://www.cog-genomics.org/plink2)


### Running the program:

The software (Plink) is already pre-installed on the server, you just have to load it

First load the module ```bioinfo-tools```

```module load bioinfo-tools```

Try it:
```
module load plink/1.90b4.9
```
and then to run:
```
plink
```

### This is what a basic command structure looks like in Plink: 
``` 
plink --filetypeflag filename --commandflag commandspecification --outputfilecommand --out outputfilename
```
For example to do filtering of missing markers at 10% frequency cutoff:
```
plink --bfile file1 --geno 0.1 --recode --out file2
```
What the function above does - reading-in a bed format file called file1, doing the filtering, writing to a ped format file called file2.

### Input formats:
File format summary:

ped format: usual format (.ped and .fam)
.ped contains marker and genotype info
.fam files contain sample info

bed format: binary/compact ped format (.fam .bim and .bed)
.fam - sample info 
.bim - marker info  
.bed - genotype info in binary format

tped format: transposed ped format (.tfam and .tped files)
.tfam - sample info, 
.tped - marker and genotype info in transposed format

### The course material can be found at the following path:
```
/proj/uppmax2023-2-1/MYSTERY_QUEST
```
So, first thing's first - create a directory for yourself in which you will be working in:
```
mkdir your_unique_name
```
Copy the datasets from the directory “1_STARTING_POINT” to your working folder by **changing** the command below while you are in your working folder. The **dot** means copy the contents "here". Do this for all your datasets. 

*Advice - it might be easier for you if all your datasets to be in the same directory (.bed +.bim +.fam files for all four, instead of having to go in and out of the different folders). (If you're copying the entire directory you can use the -r flag).

```
cp /full_path_to_course_material/unk1.bed .
cp /full_path_to_course_material/unk1.bim .
cp /full_path_to_course_material/unk1.fam .
```
**The files above contain SNPs from 4 unknown population groups in bed file format. You are going to figure out the ancestry of these population groups during this practical.**

To speed things up for you we are only working with chromosomes 20-22.
 
Look at the `.bim` (markers) and `.fam` (sample info) files by typing:

```
less unk1.bim
``` 
do the same for the `.fam` file

```
less unk1.fam 
```
As mentioned before the `.bim` file store the variant markers present in the data and `.fam` lists the samples. 
You can try to look at the `.bed` as well, but this file is in binary format and cannot be visualized as text. If you want to see the specific genotype info you must export the bed format to a ped format.

And read in a  bed file dataset and convert to ped format by typing/pasting in:

```
plink --bfile unk1 --recode --out unk1_ped 
```

### Look at the info that Plink prints onto the screen. 

How many SNPs are there in the data? How many individuals? 

Look at the first few lines of the newly generated .map (sample info) and .ped (marker and genotype info) files using the more command as demonstrated above

Read in bed/ped file and convert to tped:

```
plink --bfile unk1 --recode transpose --out unk1_tped 
plink --file unk1_ped --recode transpose --out unk1_tped 
```
Did you notice the difference in the two commands above for reading from a bed (--bfile) and reading from a ped (--file) file? Which one takes longer to read-in? 
Perhaps not too evident, since the files were not that big but compare their sizes with ```ls -lh * ``` and see if there is a difference.

You can look at the first few lines of the  `.tfam` and `.tped` files by using the `less` command
Can you see what symbol is used to encode missing data?

Note- try to always work with bed files, they are much smaller and takes less time to read in. 

Plink can convert to other file formats as well, you can have a look in the manual for the different types of conversions

# PART 1: Merging, Filtering, QC steps

## Step 1 - Filtering for missing data

First, we filter for marker missingness:

Paste in the command below to filter out markers with more than 10% missing data

```
plink --bfile unk1 --geno 0.1 --make-bed --out unk2 
```

Look at the screen output, how many SNPs were excluded?

## Step 2 - Filter for individual missingness

Paste in the command below to filter out individual missingness

```
plink --bfile unk2 --mind 0.15 --make-bed --out unk3 
```

Look at the screen output, how many individuals were excluded?

## Step 3 - MAF filtering

To filter for minimum allele frequency is not always optimal, especially if you are going to merge your data with other datasets in which the alleles might be present. But we will apply a MAF filter in this case

Filter data for a minimum allele frequency of 1% by pasting in:

```
plink --bfile unk3 --maf 0.01 --make-bed --out unk4 
```

How many SNPs are left at this point?

## Step 4 - Filtering for SNPs out of Hardy-Weinberg equilibrium

Most likely, SNPs out of HWE usually indicates problems with the genotyping. However, to avoid filtering out SNPs that are selected for/against in certain groups (especially when working with case/control data) filtering HWE per group is recommended. After, only exclude the common SNPs that fall out of the HWE in the different groups - (OPTIONAL). But for reasons of time, we will now just filter the entire dataset for SNPs that aren’t in HWE with a significance value of 0.001

```
plink --bfile unk4 --hwe 0.001 --make-bed --out unk5
```

Look at the screen. How many SNPs were excluded?

If you only what to look at the HWE stats you can do as follows. By doing this command you can also obtain the observed and expected heterozygosities. 

```
plink --bfile unk5 --hardy --out hardy_unk5
```
Look at file hardy_unk5.hwe, see if you understand the output?

This file has the following format:
```
     SNP             SNP identifier
     TEST            Code indicating sample
     A1              Minor allele code
     A2              Major allele code
     GENO            Genotype counts: 11/12/22 
     O(HET)          Observed heterozygosity
     E(HET)          Expected heterozygosity
     P               H-W p-value
```
For case/control samples, each SNP will have three entries (rows) in this file, with TEST being either ALL, AFF (cases only) or UNAFF (controls only).

There are additional filtering steps that you can go further. PLINK site on the side lists all the cool commands that you can use to treat your data. Usually, we also filter for related individuals and do a sex-check on the X-chromosome to check for sample mix-ups. 

## Step 5 - Filtering out related individuals (OPTIONAL - we can skip this for now)

In the `SCRIPTS` folder, there is a script called `sbatch_KING.sh` that can be used to run [KING](http://people.virginia.edu/~wc9c/KING/manual.html) You can have a look inside for instructions on how to run the script. Check out the manual and try and figure out how the software works.
After you have run the script have a look at the produced output files and figure out how to remove the related individuals. (*Hint - keep via Plink might be a good option) 

The input files for KING need to be in PLINK binary format, which include a binary genotype file, a family file, and a map file, e.g., ex.bed, ex.fam, and ex.bim. 
A binary format allows efficient compression of genotype data by using two bits to represent a genotype, which offers substantial computational savings that are essential to KING analysis.

First load ```bioinfo-tools``` and then ```module load KING```.

Examples of reading in a dataset are:
```
 king -b ex.bed --related
 king -b ex.bed --fam ex.fam --bim ex.bim --related
```
In the first example, although only ex.bed is specified, the other two input files are pre-assumed to be ex.fam and ex.bim. 
In the case where the other two input files may have a different prefix, the second example can be used instead. 

What you can do is save this short script and submit it as a job.
```
#!/bin/bash -l
#
#
#SBATCH -J king
#SBATCH -t 12:00:00
#SBATCH -A uppmax2023-2-1
#SBATCH -n 8
king -b $1  --unrelated
```
You can submit it as a job by doing:

```
sbatch -M snowy THIS_SCRIPT.sh YOUR_DATASET5.bed
```

Look at the output from KING & keep the unrelated individuals.

## Step 6 - Plotting the heterozygosities per population (OPTIONAL - you can look at the different heterozygosities in the different populations, perhaps skip for now)

Compare the expected heterozygosities of the four populations:
Do HWE for Unknown populations 1 - 4 

```
#getting 4 different groups from the fam files 
grep 'Unknown1 ' unk4.fam >list1 
grep 'Unknown3 ' unk4.fam >list2 
grep 'Unknown5 ' unk4.fam >list3 
grep 'Unknown11 ' unk4.fam >list4 
#extracting the groups from the bed files using the above generated lists 
plink --bfile unk4 --keep list1 --make-bed --out unk4_1 
plink --bfile unk4 --keep list2 --make-bed --out unk4_2 
plink --bfile unk4 --keep list3 --make-bed --out unk4_3 
plink --bfile unk4 --keep list4 --make-bed --out unk4_4 
#doing hwe filtering at p value <= 0.01 for the different sets of pops 
plink --bfile unk4_1 --hwe 0.01 --make-bed --out unk4_1h 
plink --bfile unk4_2 --hwe 0.01 --make-bed --out unk4_2h 
plink --bfile unk4_3 --hwe 0.01 --make-bed --out unk4_3h 
plink --bfile unk4_4 --hwe 0.01 --make-bed --out unk4_4h
```

To prepare your files for R simply paste this in:

```
plink --bfile unk4_1h --hardy --out hardy_unk4_1h 
grep 'ALL' hardy_unk4_1h.hwe | sed 's/ALL/POP1/g' >hzt
plink --bfile unk4_2h --hardy --out hardy_unk4_2h 
grep 'ALL' hardy_unk4_2h.hwe | sed 's/ALL/POP2/g' >>hzt
plink --bfile unk4_3h --hardy --out hardy_unk4_3h 
grep 'ALL' hardy_unk4_3h.hwe | sed 's/ALL/POP3/g' >>hzt
plink --bfile unk4_4h --hardy --out hardy_unk4_4h 
grep 'ALL' hardy_unk4_4h.hwe | sed 's/ALL/POP4/g' >>hzt
```

Now we will plot the heterozygosities of the two populations in R

Open R by typing

```
R
```

Paste in the following script:

```
WD<-getwd()
setwd(WD)
infile1<-"hzt"
outname<-"HeterozygosityPlot1"
data1<-read.table(infile1)
pdf (file=paste(outname,".pdf", sep =""), width =5, height = 5, pointsize =10)
boxplot(data1[,8]~data1[,3], col = "Gold", xlab="Population", ylab="Exp Heterozygosity", main = "Heterozygosity boxplot")
dev.off()
#quit R
quit()
N
```

Look at the produced PDF, do you see a difference in heterozygosity (HeterozygosityPlot1.pdf) in the populations, are they significant?

## Step 7 - Data Merging & strand flipping (Very useful but if you're really behind ask the TA if you can skip it)

The next step would be to start merging your data with comparative datasets. Due to time constraints, we are now skipping this step and continuing to exercise 5. You can, however, work through this exercise optionally.
Usually, when you merge your data with another dataset there are strand issues. The SNPs in the other dataset might be typed on the reverse DNA strand and yours on the forward, or vice versa. Therefore you need to flip the strand to the other orientation for all the SNPs where there is a strand mismatch. One should not flip C/G and A/T SNPs because one cannot distinguish reverse and forward orientation (i.e. C/G becomes G/C unlike other SNPs i.e. G/T which become C/A). Therefore before merging and flipping all A/T and C/G SNPs must be excluded. However, this can be a problem since some of your SNPs in your dataset may be monomorphic when you don't apply the MAF filter. I.E in the bim file they will appear as C 0 (with 0 meaning missing). So you don't know what kind of SNP it is, it can be C G or C T for instance if it is C G it needs to be filtered out but not if it is C T.

Therefore, before merging our data to other datasets it is important to first merge your data with a fake / reference_individual, that you prepare, which is heterozygous at every SNP position. This “fake” reference individual you can easily prepare from the SNP info file you get from the genotyping company or your own genomics processing software (such as Genome Studio from Illumina). You can also prepare it from data downloaded for each SNP from a web-database such as dbSNP. 

So our first step will be merging with a reference that we prepared form SNP info data beforehand:

For this you should have the `RefInd` files (bim, bed, and fam) from the DATA folder to your working folder.

Extract your SNPs of interest from the RefInd (remember you filtered out several SNPs already)

```
plink --bfile RefInd1 --extract unk5.bim --make-bed --out RefInd1_ext 
```

Make a list of CG and AT SNPs in your data:

```
sed 's/\t/ /g' RefInd1_ext.bim | grep " C G" >ATCGlist
sed 's/\t/ /g' RefInd1_ext.bim | grep " G C" >>ATCGlist
sed 's/\t/ /g' RefInd1_ext.bim | grep " A T" >>ATCGlist
sed 's/\t/ /g' RefInd1_ext.bim | grep " T A" >>ATCGlist
```

Exclude the CG and AT SNPs form both your reference ind and data

```
plink  --bfile RefInd1_ext --exclude ATCGlist --make-bed --out RefInd1_ext2 
plink  --bfile unk5 --exclude ATCGlist --make-bed --out unk6
```

Merge with RefInd:

```
plink --bfile RefInd1_ext2 --bmerge unk6.bed unk6.bim unk6.fam --make-bed --out MergeRef1  
```

An error is generated because of the strand mismatches. The generated file MergeRef1.missnp
contains the info on the SNPs where there are mismatches - flip the strand of these SNPs in your data.

```
plink --bfile unk6 --flip MergeRef1-merge.missnp --make-bed --out  unk7  
```

Try merging again:


```
plink --bfile RefInd1_ext2 --bmerge unk7.bed unk7.bim unk7.fam --make-bed --out MergeRef2  
```

Now it works right?

Next, we will merge our data with a set of reference populations that we get from an already published study such as HapMap data or HGDP population data. Many of the sites archiving the data provided them in PLINK format as well. For this practical, we selected a few Ref pops from HapMap and HGDP to compare your Unknown populations to.

In order to do this you need the RefInd files (bim, bed and fam) to be in your working folder.

Look at the .fam file, do you recognize some of these population names (You can use Google)?  
There are two HapMap and three HGDP populations.

First, extract the SNPs we have in our data from the downloaded RefPops

```
plink --bfile refpops1 --extract MergeRef2.bim --make-bed --out refpops2  
```

Now we will merge our data with the downloaded data

```
plink --bfile MergeRef2 --bmerge refpops2.bed refpops2.bim refpops2.fam --make-bed --out MergeRefPop1  
```

Another strand issue, flip the strands of the refPop datasets to be merged

```
plink --bfile refpops2 --flip MergeRefPop1-merge.missnp --make-bed --out refpops3  
```

Try merge again:

```
plink --bfile MergeRef2 --bmerge refpops3.bed refpops3.bim refpops3.fam --make-bed --out MergeRefPop2 
```

It works now. Look at your screen output. You will see that the Refpops only contains SNPs that overlap with a small percentage of the SNPs in the Unknown Pops data (~15 000 vs ~95 000). We will now again filter for SNP missingness to exclude all of the extra SNPs in the Unknown Pop data (Retain only the overlap).

```
plink --bfile MergeRefPop2 --geno 0.1 --make-bed --out MergeRefPop2fil 
```

How many SNPs are left for your analyses?

The last thing to do is to extract your ***fake/Ref_ind*** from your data.

```
plink --bfile MergeRefPop2fil --remove RefInd1.fam --make-bed --out MergeRefPop3  
```

This is the final files for the next exercise. Rename them:

```
mv MergeRefPop3.bed PopStrucIn1.bed; mv MergeRefPop3.bim PopStrucIn1.bim; mv MergeRefPop3.fam PopStrucIn1.fam 
```
## Step 8 - LD Pruning
Before running PCA & ADMIXTURE it is advisable to prune the data to thin the marker set for linkage disequilibrium.
In a nutshell - it uses the first SNP (in genome order) and computes the correlation with the following ones (e.g. 50). When it finds a large correlation, it removes one SNP from the correlated pair, keeping the one with the largest minor allele frequency (MAF), thus possibly removing the first SNP. Then it goes on with the next SNP (not yet removed). You can read up on how to prune for LD (https://dalexander.github.io/admixture/admixture-manual.pdf).

```plink --bfile YOUR_DATASET --indep-pairwise 10 10 0.1
   plink --bfile YOUR_DATASET --extract plink.prune.in --make-bed --out YOUR_DATASET_PRUNED
```
## Step 9 - Reflect and review

How many SNPs are left for your analyses? 
What was the genotyping rate for the combined reference dataset and what was it for the unknowns? 
Finally what is the genotyping rate for the final merged dataset? 

This are the final files for the next exercise. Now you have generated your input files for the next part which will deal with population structure analysis. You will look at the population structure of your unknown samples in comparison to the known reference populations that came from HapMap and HGDP.

=============================================================================

# PART 2: Population structure inference 

For this part of the exercize copy the files from ```/proj/uppmax2023-2-1/MYSTERY_QUEST/2_POPULATION_STRUCTURE_INFERENCE/2_PCA``` to your directory.

## Step 1 - Principal component Analysis with Eigensoft

The first population structure method we will look at is Principal Components Analysis with Eigensoft. 

If we had a longer time to work on this exercize you would run a PCA analysis as explained below. Since we have one afternoon, it is good to look at the steps, understand them and maybe try them at home. After reading the steps, you can jump straight to the plotting step.

### Running the PCA analysis - we are skipping this step today, but feel free to try at home:

You need to run eigensoft on the .bed format from plink. You only need to modify your .fam file a little for it to work in Eigensoft. The .bed and .map files you use directly as-is. The .fam file you change the extension to .pedind and you substitute the last column (-9 at the moment indicating missing phenotype) with population numbers. When assigning pop numbers do not use 1, 2 or 9. They are reserved for cases, controls and missing data in Eigensoft.

Paste this piece of code in to the terminal:

```
cut -d " " -f1-5 PopStrucIn1.fam >file1a
cut -d " " -f1 PopStrucIn1.fam >file2a
sed "s/Unknown1/51/g" <file2a | sed "s/Unknown3/53/g" | sed "s/Unknown5/55/g" | sed "s/Unknown11/61/g" | sed "s/CEU/81/g" | sed "s/YRI/82/g" | sed "s/Han/83/g" | sed "s/San/84/g" | sed "s/MbutiPygmies/85/g" >file3a
paste file1a file3a >fileComb
sed "s/\t/ /g" fileComb > PopStrucIn1.pedind
rm file1a; rm file2a; rm file3a; rm fileComb
```
It will make a `.pedind` file from your `.fam` file.

Furthermore, you need a parameter file to indicate your parameter options to EIGENSOFT.

Copy the prepared parameter file from the `DATA` directory to your working folder it's called 
`PopStrucIn1.par`

Open the parameter file and look at what is specified in it. At the start is the input/output files. Furthermore, we ask for the info for 10 PCs to be output, qtmode is set to NO to indicate more than one pop, we prune the SNPs based on LD for an r2 value of 0.2. We don't remove any outlying points and we limit the sample size to 20. This is important for PCA where there are groups with very large sample sizes since large sample sizes will distort the PC plot. It is best for PCA that sample sizes are as even as possible.

Run the smartpca package in Eigensoft by typing

```
smartpca -p PopStrucIn1.par
```
### Plotting the PCA in R (Doing this today)

The output files from the PCA analysis are in the ```.evec``` and ```.eval``` format.

In the `.evec` file is the main output for the number of PCs that you specified in the .par file. The first row is the Eigenvalues for each of your PCs the rest of the rows list your Pop:Ind specification, the PCs and their loadings and your PopNumber at the end. 

In the `.eval` is all the eigenvalues that were extracted. To work out the percentage of variation each PC explains, you divide your particular PC eigenvalue by the sum over all the eigenvalues.

Now, prep for R:

```
module load R/3.4.3
```

```
sed 1d PopStrucIn1.evec | sed  "s/:/   /g " >   PopStrucIn1.evecm
```

Open R by:
```
R
```
See if you understand the code below. It basically plots PC1 vs PC2 (and so forth) and puts the right labels on the plot.
After you've opened `R`, paste the following code to plot your PCs:

```
WD<-getwd()
setwd(WD)
## Define these
evec<- read.table ("PopStrucIn1.evecm")
eval<- read.table ("PopStrucIn1.eval")
namer <- "PopStrucIn1"
nrpc<-10
## Script start
totalev <-sum(eval)
aa <- array(NA,dim=c(nrpc,1))
for (i in 1:nrpc) {
    aa[i,1]<-format(round(((eval[i,1]/totalev)*100),3), nsmall = 3)}
pdf (file =paste(namer, "_PCA1.pdf", sep=""), width =10, height = 15, pointsize =12)
par(mfrow=c(3,2), oma=c(0,0,4,0))
plot (evec[,3], evec[,4],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC1: ", aa[1,1], "%", sep=""), ylab=paste("PC2: ", aa[2,1], "%", sep=""))
legend("topright",  legend = unique(evec[,1]), text.col = "black", cex = 0.75, pch =unique(evec[,1]), col = unique(evec[,1]), xpd = TRUE, bty="n", )
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
plot (evec[,5], evec[,6],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC3: ", aa[3,1], "%", sep=""), ylab=paste("PC4: ", aa[4,1], "%", sep=""))
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
plot (evec[,7], evec[,8],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC5: ", aa[5,1], "%", sep=""), ylab=paste("PC6: ", aa[6,1], "%", sep=""))
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
plot (evec[,9], evec[,10],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC7: ", aa[7,1], "%", sep=""), ylab=paste("PC8: ", aa[8,1], "%", sep=""))
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
plot (evec[,11], evec[,12],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC9: ", aa[9,1], "%", sep=""), ylab=paste("PC10: ", aa[10,1], "%", sep=""))
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
barplot (as.numeric(aa[,1]), xlab = "PC", ylab = "%Variation explained", axes=TRUE)
title(paste(namer, "PC plot"), outer=TRUE, cex.main = 1)
dev.off()
q()
```
Once the plot is done, and it will be very quick, click ```Enter``` and you will be asked the following:

```
Save workspace image? [y/n/c]:
```
Type `y` and press enter and a PDF will have been generated in the directory you are working in. 
All you need to do now, is to copy the PDF file over to your computer. 
(In order to view it you need to use a X11 connection, but the datalabs should be equipped with it.)

Copying the file from the server onto your computer can be done via ```scp```:

```
scp YOUR_USERNAME@rackham.uppmax.uu.se:/crex/proj/uppmax2023-2-1/MYSTERY_QUEST/YOUR_DIRECTORY/PopStrucIn1_PCA1.pdf . 
```
After which you will be asked to type in your password again.

Look at the output PDF. Do the results of your population PCA correspond to the population structure results you got from the ADMIXTURE plots? How many of the PCs do you think contain useful information. What part of the variation is represented by each of the PCs. Can you see the percentage variation that each PC explains?

### Projected PCA (Skipping this today, again, feel free to try at home)

We will set-up a projected PCA run in Eigensoft so that the PCs are calculated based on the Ref pops only and the Unknown individuals are projected on these PCs based on the Ref pops.

To do that we need a slightly modified .par file and an additional file that lists the pops to use as ref pops. Copy these two files from the `SCRIPTS` folder and open them to see how they differ from the .par file used above:
`PopStrucIn1_Proj.par`
RefGroups.groups

Run Eigensoft by typing:

```
smartpca -p PopStrucIn1_Proj.par
```

Prepare the output files for R and run the R script as explained above. Remember the output filenames changed thus adapt the scripts used above accordingly (both the bash script line and the first part of the R script enclosed by the “”)

Look at the output PDF, what has changed? This should give you a further indication as to who your Unknown populations are.


Lastly, we will look at which SNPs contribute to which axes in the PC (SNP weightings of each principal component). This is one way to identify the most informative SNPs that define the structure between certain populations. This is useful if you want to look at population structure but you only want to pick a few best SNPs to type. To generate such a list you just add the snpweightoutname to your par file. We will also not prune for LD so that we do not exclude possible informative SNPs
 
Copy the modified .par file look how it looks and run Eigensoft
PopStrucIn1_snpweight.par

```
./smartpca -p PopStrucIn1_snpweight.par
```

Look at the `PopStrucIn1.snpweight_out` file. The file contains a list of snps and the weights they have on each PC. You can easily select and sort the list to obtain the SNPs with max info for a given PC.

Look at your previous generated PC plot `PopStrucIn1_Proj_PCA1.pdf`
Axis one (PC1) explains ~8% of the variation in your whole Ref_pop dataset. It is the axis that defines the difference between African and non-African populations. To generate a sorted list of the SNPs that would be the best to type to look at the difference between African and non-Africans paste the following command:

```
sed 's/ \+ /\t/g' PopStrucIn1.snpweight_out | sed 's/^\t//g' | cut -f1,3 | sort -r -n -k 3,3  >topSNPsPc1
```

Look at the topSNPsPc1 file that is generated


If you are interested in the frequency of the top SNP in your data. Copy the two scripts below:

```
Extract_snp_from_bed_make_Barplot
Extract_snp_from_bed_make_Barplot.R
```

Open the main script and paste the rs name of the SNP you are interested in in the place where rs00000000 is at the moment. Save the script and run by typing:

```
./Extract_snp_from_bed_make_Barplot
```

## Step 3 - ADMIXTURE 

ADMIXTURE is a clustering software similar to STRUCTURE with the aim to infer populations and individual ancestries. 

### Same as with the PCA, today we are skipping the analysis part, and going directly to the plotting - Step 4

ADMIXTURE runs directly from .bed or .ped files and needs no extra parameters for file preparation. 
You do not specify burin and repeats, ADMIXTURE exits when it converged on a solution (Delta< minimum value)

A basic ADMIXTURE run looks like this:

```
admixture -s time YOUR_PRUNED_DATASET.bed 2
```
This command executes the program with a seed set from system clock time, it gives the input file (remember the extension) and the K value at which to run ADMIXTURE (2 in the previous command).

For ADMIXTURE you also need to run many iterations at each K value, thus a compute cluster and some scripting is useful.

First, you have to load the module:

 ```
 module load bioinfo-tools
 module load ADMIXTURE/1.3.0
 ```

Make a script from the code below to run Admixture for K = 2-6 with 3 iterations at each K value.

```
#!/bin/bash
for kval in {2..6}; do
   for itir in {1..3}; do
           (echo '#!/bin/bash -l'
           echo "
           mkdir ${kval}.${itir}
           cd ${kval}.${itir}
#working folder where admixture and the bed files are
module load bioinfo-tools
module load ADMIXTURE/1.3.0
admixture -j3 -s $RANDOM path/to/your/YOUR_PRUNED_DATASET.bed ${kval}
cd ../
rm -r ${kval}.${int}
#moves it to a different folder and renames it so you end up with multiple iterations
exit 0") |
               sbatch -M snowy -p core -n 3 -t 72:0:0 -A YOUR_UPPMAX_PROJECT_HERE -J ADMX.${kval}.${itir} -o ADMX.${kval}.${itir}.output -e ADMX${kval}.${itir}.output
       done
done
```

The **project name** and the **path** to the pruned .bed-file need to be replaced in the script before running it:

```
bash NAME_OF_THE_SCRIPT.sh
```
After a successful Admixture run, you should be seeing a bunch of new folders being created. 
In each one of them you will find the output ```P``` and ```Q``` for each iteration. 

## Step 4 - Visualizing the results from ADMIXTURE - PONG 

When the admixture analysis is finally done we use PONG to combine the different iterations and visualize our results. 

Copy the files from the ```1_ADMIXTURE_RUNS/``` directory to your working directory as follows:
```
cp -r full_path_to/1_ADMIXTURE_RUNS/ full_path_to_your_directory/
```

As always, first load the module:
 ```
 module load pong 
 ```
To be able to run PONG we thus need to generate **three different files**.

The first being the ***filemap***. This is the only input that is strictly required to run PONG. It consists of three columns.

From the PONG manual: 

```
Column 1. The runID, a unique label for the Q matrix (e.g. the string “run5_K7”).

Column 2. The K value for the Q matrix. Each value of K between Kmin and Kmax must
be represented by at least one Q matrix in the filemap; if not, pong will abort.

Column 3. The path to the Q matrix, relative to the location of the filemap. 
```
In order to create what we need we can edit and run the loop below. Make sure to edit the right number of **k** and **i**.
Also check whether the prefix of the admixture output Q files is the same in both the loop and the Q files. As long as you are starting PONG in the same directory of all the Admixture output files you should be ok.

```
for i in {2..10};
do
    for j in {1..15};
    do
    echo -e "k${i}_r${j}\t${i}\t${i}.${j}/YOUR_DATASET.${i}.Q" >> unknown_FILEMAP.txt
    done
done

```
Example of what this file looks like, depending on how many K and how many iterations you run:

```
k2_r1	2	2.1/YOUR_DATASET.2.Q
k2_r2	2	2.2/YOUR_DATASET.2.Q
k2_r3	2	2.3/YOUR_DATASET.2.Q
k3_r1	3	3.1/YOUR_DATASET.3.Q
k3_r2	3	3.2/YOUR_DATASET.3.Q
k3_r3	3	3.3/YOUR_DATASET.3.Q
k4_r1	4	4.1/YOUR_DATASET.4.Q
k4_r2	4	4.2/YOUR_DATASET.4.Q
k4_r3	4	4.3/YOUR_DATASET.4.Q
k5_r1	5	5.1/YOUR_DATASET.5.Q
k5_r2	5	5.2/YOUR_DATASET.5.Q
k5_r3	5	5.3/YOUR_DATASET.5.Q
k6_r1	6	6.1/YOUR_DATASET.6.Q
k6_r2	6	6.2/YOUR_DATASET.6.Q
k6_r3	6	6.3/YOUR_DATASET.6.Q
k7_r1	7	7.1/YOUR_DATASET.7.Q
k7_r2	7	7.2/YOUR_DATASET.7.Q
k7_r3	7	7.3/YOUR_DATASET.7.Q
```

The next file we need to create is the ***ind2pop*** file. It is just a list of which population each individual belongs to.
We have this information in the `.fam` file so we can just cut out the field we need:

```
cut -f 1 -d " " YOUR_DATASET.fam > unknown_IND2POP.txt

``` 
The ind2pop file should look exactly like the first column of the fam.

The ***poporder*** file is a key between what your populations are called and what "Proper" name you want to show up in your final plot. 
Also as the name suggests, **gives the order in which you want the populations to be in**, so order them as you think it makes most sense.

*Note that the file needs to be **tab-delimited**, i.e separated by tabs (If you have spaces issues may occure). 
Also, make sure within the names in this file to not have any spaces. If you do, try replacing them with underscores (_).*
*Again, be careful when editing this file, typos, extra spaces etc might cause problems.*

Below is just an example of what a poporder file should look like:

```
aDNA_Rick	pickle
aDNA_Morty	m0rty
aDNA_Ragnar	rgnr
aDNA_martians	mrt99
Napoleon_Bonaparte	nb001
Netherlands_BA	Dutch
```
An easy way of doing this is by :
```
cut -f 1 -d " " YOUR_DATASET.fam | uniq > unknown_POPORDER_prep.txt
```
and then just save and run this simple script:
```
#!usr/bin/env bash

file=$(cat unknown_POPORDER_prep.txt)

for line in $file
do
    echo -e "$line\t$line" >> unknown_POPORDER.txt
done
```
This way you will get a file with two columns. You can edit the the names of the populations you are interested in by changing the names of the second column.

### Now we have all the files we need. Time to run PONG.

You are working on the datalab computers and PONG can be a bit slow and clunky - but should be good enough for today.

In case anyone is working from home - it is by far best to run PONG on your local computer. All you need to do is install PONG and copy the ```.Q``` files to your computer together with the three files (filemap,ind2pop,poporder) and run it. If you don't know how to install PONG(https://github.com/ramachandran-lab/pong), it is available through the module system on Uppmax, although very often it is quite laggy. 

* Mac Users - you will need to install XQuartz (an open-source version of the X.Org X server, a component of the X Window System that runs on macOS)https://www.xquartz.org
* Windows Users - MobaXterm https://mobaxterm.mobatek.net
* Ubuntu Users - I'm sure you can figure it out yourselves

### Super important:
Since we are several people who are going to run PONG at the same time we need to use a different port, otherwise, we will collide with each other. The default port for PONG is 4000. Any other free port will work, like 4001, 2, etc. Make sure you are using a **unique port** before proceeding. If multiple people are trying to run PONG on the same port you have problems, so talk to your classmates and find a unique port in the 4000s that people aren't using.

```
pong -m unknown_FILEMAP.txt -i unknown_IND2POP.txt -n unknown_POPORDER.txt -g --port YOUR_PORT_NUMBER_HERE
```

When PONG is done it will start hosting a webserver that displays the results at port 4000 by default:  ```http://localhost:4000```. Pong needs to be running for you to look at the results, i.e. **if you close it it will not work.**
It will look like this:

```
-------------------------------------------------------------------
                            p o n g
      by A. Behr, K. Liu, T. Devlin, G. Liu-Fang, and S. Ramachandran
                       Version 1.5 (2021)
-------------------------------------------------------------------
-------------------------------------------------------------------

Parsing input and generating cluster network graph
Matching clusters within each K and finding representative runs
For K=2, there are 3 modes across 3 runs.
For K=3, there are 3 modes across 3 runs.
For K=4, there are 3 modes across 3 runs.
For K=5, there are 3 modes across 3 runs.
For K=6, there are 3 modes across 3 runs.
For K=7, there are 3 modes across 3 runs.
Matching clusters across K
Finding best alignment for all runs within and across K
match time: 0.64s
align time: 0.01s
total time: 1.21s
-----------------------------------------------------------
pong server is now running locally & listening on port 4005
Open your web browser and navigate to ```http://localhost:4005``` to see the visualization
```

The web server is hosted on the **same login node as you were running pong**. In case you are unsure of which one that is you can use the command `hostname` to figure it out:

```
hostname
```

To view files interactively you need to have an X11 connection. So when you connect to the server from a **new tab** do:

```
ssh -AY YOUR_USERNAME_HERE@rackham.uppmax.uu.se
```

Make sure that you connect to **the same login (i.e 1,2,3 etc) as you got from hostname**. (Ex. ```YOUR_USERNAME_HERE@rackham2.uppmax.uu.se```)

In a **new tab** (if you didn't put PONG in the background) type:

```
firefox http://localhost:YOUR_PORT_NUMBER
```

Once you are finished with looking at the PONG output you can click and save some output to file and then close the program by hitting `ctrl - c` in the terminal tab running it. 

### What do the results from Admixture & PCA tell you? 
What do you see? What information does this give you about your mysterious unknown populations? 
Do the results of your PCA correspond to the population structure results you got from the ADMIXTURE plots? Can you figure out who they are?

**After you have finished with your analyses, delete all unnecessary files from your folder.**

# Part 3: Review & reflect

Okay, hopefully this project wasn't too difficult or too boring or too fast. Maybe it was even fun? 
The point of today's exercize was to get a broad understanding of what a population structure analysis looks like (at least in humans!) and get a visual representation of some results. Of course, there is plenty more to do, many analyses to run and lots more data to be scrutinized but hopefully by going over some of the things today it got your imagination tingling! If you want to know more, you can read up on the papers recommended above. Although the population history of the continent we just worked on is very very complex, they should provide you with enough of a backstory to be able to grasp the grand picture of things. You are free to also look into similar papers on the subject. Until next time!
 
