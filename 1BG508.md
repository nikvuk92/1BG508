# Populationgenomik 1BG508

## Instructions for project work

The goal of this population genomics project is to familiarize you with the workflows, different types of commonly used analyses as well as a way of thinking about how one approaches questions such as trying to explain the underlying forces that affect populations in general. We know that people tend to migrate and throughout history they have done that a lot. Similar populations get isolated and with time may become more distant, distant peoples admix with time may become more closely related - and trying to decipher human population history is usually not an easy task.

Since we only have one afternoon to work with, the structure of the project work will be organized as follows: 

- each group consists of three students;
- you need an username & password to log into Uppmax;
- up to you to decide if you want to work on your own laptops or in Hubben;
- we have an introductory meeting where we discuss the framework of the exercize in depth;
- in the few weeks you have until the course ends feel free to send in emails with specific questions if something is not working (or you're just curious);
- we can also set up meeting times that work for all of us;
- the end product should be a Scientific report of ~10 pages, including Abstract, Introduction, Materials & Methods, Results, Discussion, References, Appendices (length is not crucial here, substance is);
- A short presentation (date will be set);
- the deadline for the written report is the day of the presentation.

## Unmasking the mystery of the four populations

The specific point of the exercise is for you to identify the populations you have been given in the **'unk1.fam'**. In order to achieve this, you have been provided a bunch of references that should come in handy as a way to compare the unknown to the known. Both reference and target populations can be found in the DATA directory:
```
/proj/uppmax2022-2-21/PGA_2022/DATA/Reference_datasets/
/proj/uppmax2022-2-21/PGA_2022/DATA/Mystery_populations
```
Helpful scripts are there to aid you on your quest can be found:
```
/proj/uppmax2022-2-21/PGA_2022/SCRIPTS
```
In the steps that will lead you towards unmasking the unknown populations you will need to: 

1. Filter the **reference** individuals & SNPs (missingness, HWE, MAF) - *word of caution: do not apply the 'maf' filtering until you have your final references dataset!*;
   *Remember: The **unknown** samples **do not** go through any filtering!*
2. After filtering - merge the datasets together;
3. Before running PCA/Admixture you need to prune your data for SNPs in LD (pruning is explained in the admixture manual https://dalexander.github.io/admixture/admixture-manual.pdf);
4. Plot a PCA
5. Plot the Admixture results
6. Read up on some literature - it will help for writing the report! (Pay special attention to the key figures in them):
[Schlebusch 2012](https://pubmed.ncbi.nlm.nih.gov/22997136/)
[Gurdasani_2015](https://www.nature.com/articles/nature13997) 
[Patin 2017](https://science.sciencemag.org/content/356/6337/543)
[Vicente_2021](https://bmcbiol.biomedcentral.com/articles/10.1186/s12915-021-01193-z)

## Some technicalities

* You should have R studio installed on your computer

Before we start here are some basic Unix/Linux commands if you are not used to working in a Unix-style terminal:

### Logging into SNOWY & working on the server:

```
ssh -AX user@rackham.uppmax.uu.se
```
After which you type in your password. *And yes, it'll not show up when typing it, but if you type it in correctly & press enter, you're in.*

Note: When acessing SNOWY from Rackham's login nodes you must always use the flag -M for all SLURM commands.
Examples:

```
- squeue -M snowy
- jobinfo -M snowy
- sbatch -M snowy slurm_script_file
- scancel -u username -M snowy
- interactive -A projectname -M snowy -p node -n 32 -t 01:00:00
```

Note: It is recommended to load all your modules in your job script file. This is even more important when running on Snowy since the module environment is not the same on the Rackham login nodes as on Snowy compute nodes.
You can read up more on how to use SNOWY : [Snowy_User_guide](https://www.uppmax.uu.se/support/user-guides/snowy-user-guide/)

Generally, for any bigger job (bigger Plink jobs, PCA, Admixture) **always work in interactive mode**.

### How to run interactively on a compute node:

Ex.:
```
   interactive -A uppmax2022-2-21 -M snowy -p node -n 32 -t 02:00:00
```
### Moving about:
```
    cd – change directory
    pwd – display the name of your current directory
    ls – list names of files in a directory
```
### File/Directory manipulations:
```
    cp – copy a file
    mv – move or rename files or directories
    mkdir – make a directory
    rm – remove files or directories
    rmdir – remove a directory
```
### Display file content:
```
    cat - concatenate file contents to the screen or to a file
    less - open file for viewing
    more 
    tail
    head
    realpath - checking the path to a file 
```
### You can check your jobs with:

```
jobinfo -u YOUR_USERNAME -M snowy
```
When creating and editing a file in text editor you can use ```nano``` or ```vim``` or something else.

# PART 0 Knowing your way around & using Plink   	    

**Beware** - it is vital to not apply any filters to your unknown samples - we don't know anything about them at this point! 

FYI: Link to PLINK site:[https://www.cog-genomics.org/plink2](https://www.cog-genomics.org/plink2)

PLINK is a software for fast and efficient filtering, merging, editing of large SNP datasets, and exports to different outputs directly usable in other programs. Stores huge datasets in compact binary format from which it reads-in data very quickly.

### Running the program:

If working on Rackham type in the following:
```
module load bioinfo-tools
module load plink/1.90b4.9 

```
The software is already pre-installed on Rackham, you just have to load it

Try it:
```
plink
```

### This is what a basic command structure looks like in Plink: 
``` 
plink --filetypeflag filename --commandflag commandspecification --outputfilecommand --out outputfilename
```
For example to do filtering of missing markers at 10% frequency cutoff (reading-in a bed format file called file1, doing the filtering, writing to a ped format file called file2):
```
plink --bfile file1 --geno 0.1 --recode --out file2
```

### Input formats:
File format summary:

ped format: usual format (.ped and .fam)
.ped contains marker and genotype info and .fam files contain sample info

bed format: binary/compact ped format (.fam .bim and .bed)
(.fam - sample info  .bim - marker info  and .bed - genotype info in binary format

tped format: transposed ped format (.tfam and .tped files)

tfam sample info, .tped marker and genotype info in transposed format

lgen format: long format (see manual, not used that often)

### Setup
Navigate to the directory you want to work in.
```
cd path (change directory)
mkdir directory_name (create a new directory)
```
If you don't already have a working directory for this course then create one now:

```
cd /proj/uppmax2022-2-21 #Uppmax project for this course <<<<<<<<<<<<
mkdir your_unique_team_name
```

### The course material can be found at the following path:
```
/proj/uppmax2022-2-21/PGA_2022 <<<<<<<<<<<<<<<<<<
```

Copy the datasets from the directory “DATA” to your working folder by **changing** the command below while you are in your working folder. The **dot** means copy the contents "here". Do this for all your datasets. 

*Advice - it might be easier for you if all your datasets (.bed +.bim +.fam files for all four, instead of having to go in and out of the different folders) are in the same directory.

```
cp /full_path_to_course_material/unk1.bed .
cp /full_path_to_course_material/unk1.bim .
cp /full_path_to_course_material/unk1.fam .
```
**The files above contain SNPs from 4 unknown population groups in bed file format. You are going to figure out the ancestry of these population groups during this practical.**
To speed things up for you we are only working with chromosomes 20-22.
 
Look at the `.bim` (markers) and `.fam` (sample info) files by typing:

```
less umk1.bim
``` 
do the same for the `.fam` file

```
less unk1.fam 
```
As mentioned before the `.bim` file store the variant markers present in the data and `.fam` lists the samples. (you can try to look at the .bed as well, but this file is in binary format and cannot be visualized as text if you want to see the specific genotype info you must export the bed format to a ped format)

Read in a  bed file dataset and convert to ped format by typing/pasting in:

```
plink --bfile unk1 --recode --out unk1_ped 
```

### Look at the info that Plink prints to the screen. 
How many SNPs are there in the data? How many individuals? 

Look at the first few lines of the newly generated .map (sample info) and .ped (marker and genotype info) files using the more command as demonstrated above

Read in bed/ped file and convert to tped:

```
plink --bfile unk1 --recode transpose --out unk1_tped 
plink --file unk1_ped --recode transpose --out unk1_tped 
```
**Do you see the difference in the two commands above for reading from a bed (--bfile) and reading from a ped (--file) file. 
Which one takes longer to read-in?**

Look at the first few lines of the  `.tfam` and `.tped` files by using the `less` command

**Can you see what symbol is used to encode missing data?**

Note- try to always work with bed files, they are much smaller and takes less time to read in. 
See this for yourself by inspecting the difference in file sizes:

```
ls -lh * 
```

Plink can convert to other file formats as well, you can have a look in the manual for the different types of conversions

# PART 1: Merging, Filtering, QC steps

## Step 1 Filtering for missing data

First, we filter for marker missingness:

Paste in the command below to filter out markers with more than 10% missing data

```
plink --bfile unk1 --geno 0.1 --make-bed --out unk2 
```

Look at the screen output, how many SNPs were excluded?

## Step 2 Filter for individual missingness

Paste in the command below to filter out individual missingness

```
plink --bfile unk2 --mind 0.15 --make-bed --out unk3 
```

Look at the screen output, how many individuals were excluded?

## Step 3 MAF filtering

To filter for minimum allele frequency is not always optimal, especially if you are going to merge your data with other datasets in which the alleles might be present. But we will apply a MAF filter in this case

Filter data for a minimum allele frequency of 1% by pasting in:

```
plink --bfile unk3 --maf 0.01 --make-bed --out unk4 
```

How many SNPs are left at this point?

## Step 4 Filtering for SNPs out of Hardy-Weinberg equilibrium

Most likely, SNPs out of HWE usually indicates problems with the genotyping. However, to avoid filtering out SNPs that are selected for/against in certain groups (especially when working with case/control data) filtering HWE per group is recommended. After, only exclude the common SNPs that fall out of the HWE in the different groups - (OPTIONAL). But for reasons of time, we will now just filter the entire dataset for SNPs that aren’t in HWE with a significance value of 0.001

```
plink --bfile unk4 --hwe 0.001 --make-bed --out unk5
```

Look at the screen. How many SNPs were excluded?

If you only what to look at the HWE stats you can do as follows. By doing this command you can also obtain the observed and expected heterozygosities. 

```
plink --bfile unk5 --hardy --out hardy_unk5
```
Look at file hardy_unk5.hwe, see if you understand the output?

There are additional filtering steps that you can go further. PLINK site on the side lists all the cool commands that you can use to treat your data. Usually, we also filter for related individuals and do a sex-check on the X-chromosome to check for sample mix-ups. 

## Step 5 Filtering out related individuals (OPTIONAL - See if you have the time)

In the `SCRIPTS` folder, there is a script called `sbatch_KING.sh` that can be used to run [KING](http://people.virginia.edu/~wc9c/KING/manual.html) You can have a look inside for instructions on how to run the script. Check out the manual and try and figure out how the software works.
After you have run the script have a look at the produced output files and figure out how to remove the related individuals. (*Hint - keep via Plink might be a good option) 
*P.S. You don't need to do run KING for the Unknown dataset.*

First load ```bioinfo-tools``` and then ```module load KING```.
The script for KING is as follows:

```
#!/bin/bash -l
#
#
#SBATCH -J king
#SBATCH -t 12:00:00
#SBATCH -A uppmax2022-2-21
#SBATCH -n 8
king -b $1  --unrelated
```
You can submit it as a job by doing:
```
sbatch -M snowy THIS_SCRIPT.sh YOUR_DATASET5.bed
```
Look at the output from KING & keep the unrelated individuals.

## Step 6 OPTIONAL - If you have time plot the heterozygosities per population in R and look at the different heterozygosities in the different populations. If short on time, you can try this at home.

Compare the expected heterozygosities of the four populations:
Do HWE for Unknown populations 1 - 4 

```
#getting 4 different groups from the fam files 
grep 'Unknown1 ' unk4.fam >list1 
grep 'Unknown3 ' unk4.fam >list2 
grep 'Unknown5 ' unk4.fam >list3 
grep 'Unknown11 ' unk4.fam >list4 
#extracting the groups from the bed files using the above generated lists 
plink --bfile unk4 --keep list1 --make-bed --out unk4_1 
plink --bfile unk4 --keep list2 --make-bed --out unk4_2 
plink --bfile unk4 --keep list3 --make-bed --out unk4_3 
plink --bfile unk4 --keep list4 --make-bed --out unk4_4 
#doing hwe filtering at p value <= 0.01 for the different sets of pops 
plink --bfile unk4_1 --hwe 0.01 --make-bed --out unk4_1h 
plink --bfile unk4_2 --hwe 0.01 --make-bed --out unk4_2h 
plink --bfile unk4_3 --hwe 0.01 --make-bed --out unk4_3h 
plink --bfile unk4_4 --hwe 0.01 --make-bed --out unk4_4h
```

##### To prepare your files for R just paste this in:

```
plink --bfile unk4_1h --hardy --out hardy_unk4_1h 
grep 'ALL' hardy_unk4_1h.hwe | sed 's/ALL/POP1/g' >hzt
plink --bfile unk4_2h --hardy --out hardy_unk4_2h 
grep 'ALL' hardy_unk4_2h.hwe | sed 's/ALL/POP2/g' >>hzt
plink --bfile unk4_3h --hardy --out hardy_unk4_3h 
grep 'ALL' hardy_unk4_3h.hwe | sed 's/ALL/POP3/g' >>hzt
plink --bfile unk4_4h --hardy --out hardy_unk4_4h 
grep 'ALL' hardy_unk4_4h.hwe | sed 's/ALL/POP4/g' >>hzt
```

Now we will plot the heterozygosities of the two populations in R

Open R by typing

```
R
```

Paste in the following script:

```
WD<-getwd()
setwd(WD)
infile1<-"hzt"
outname<-"HeterozygosityPlot1"
data1<-read.table(infile1)
pdf (file=paste(outname,".pdf", sep =""), width =5, height = 5, pointsize =10)
boxplot(data1[,8]~data1[,3], col = "Gold", xlab="Population", ylab="Exp Heterozygosity", main = "Heterozygosity boxplot")
dev.off()
#quit R
quit()
N
```

Look at the produced PDF, do you see a difference in heterozygosity (HeterozygosityPlot1.pdf) in the populations, are they significant?

## Step 7 Data Merging & strand flipping (Very useful but if you're really behind ask the TA if you can skip it)

The next step would be to start merging your data with comparative datasets. Due to time constraints, we are now skipping this step and continuing to exercise 5. You can, however, work through this exercise optionally.
Usually, when you merge your data with another dataset there are strand issues. The SNPs in the other dataset might be typed on the reverse DNA strand and yours on the forward, or vice versa. Therefore you need to flip the strand to the other orientation for all the SNPs where there is a strand mismatch. One should not flip C/G and A/T SNPs because one cannot distinguish reverse and forward orientation (i.e. C/G becomes G/C unlike other SNPs i.e. G/T which become C/A). Therefore before merging and flipping all A/T and C/G SNPs must be excluded. However, this can be a problem since some of your SNPs in your dataset may be monomorphic when you don't apply the MAF filter. I.E in the bim file they will appear as C 0 (with 0 meaning missing). So you don't know what kind of SNP it is, it can be C G or C T for instance if it is C G it needs to be filtered out but not if it is C T.

Therefore, before merging our data to other datasets it is important to first merge your data with a fake / reference_individual, that you prepare, which is heterozygous at every SNP position. This “fake” reference individual you can easily prepare from the SNP info file you get from the genotyping company or your own genomics processing software (such as Genome Studio from Illumina). You can also prepare it from data downloaded for each SNP from a web-database such as dbSNP. 

Have you noticed that PLINK sometimes generates a .nof file and in the log file output the following is mentioned
902 SNPs with no founder genotypes observed
Warning, MAF set to 0 for these SNPs (see --nonfounders)
This is the monomorphic SNPs in your data.

So our first step will be merging with a reference that we prepared form SNP info data beforehand:

Copy the `RefInd` files (bim, bed, and fam) from the DATA folder to your working folder.

Extract your SNPs of interest from the RefInd (remember you filtered out several SNPs already)


```
plink --bfile RefInd1 --extract unk5.bim --make-bed --out RefInd1_ext 
```

Make a list of CG and AT SNPs in your data:


```
sed 's/\t/ /g' RefInd1_ext.bim | grep " C G" >ATCGlist
sed 's/\t/ /g' RefInd1_ext.bim | grep " G C" >>ATCGlist
sed 's/\t/ /g' RefInd1_ext.bim | grep " A T" >>ATCGlist
sed 's/\t/ /g' RefInd1_ext.bim | grep " T A" >>ATCGlist
```

Exclude the CG and AT SNPs form both your reference ind and data

```
plink  --bfile RefInd1_ext --exclude ATCGlist --make-bed --out RefInd1_ext2 
plink  --bfile unk5 --exclude ATCGlist --make-bed --out unk6
```

Merge with RefInd


```
plink --bfile RefInd1_ext2 --bmerge unk6.bed unk6.bim unk6.fam --make-bed --out MergeRef1  
```

An error is generated because of the strand mismatches. The generated file MergeRef1.missnp
contains the info on the SNPs where there are mismatches - flip the strand of these SNPs in your data.


```
plink --bfile unk6 --flip MergeRef1-merge.missnp --make-bed --out  unk7  
```

Try merging again:


```
plink --bfile RefInd1_ext2 --bmerge unk7.bed unk7.bim unk7.fam --make-bed --out MergeRef2  
```

Now it works.
No .nof file is generated which means none of your SNPs are monomorphic anymore

Now we will merge our data with a set of reference populations that we get from an already published study such as HapMap data or HGDP population data. Many of the sites archiving the data provided them in PLINK format as well. For this practical, we selected a few Ref pops from HapMap and HGDP to compare your Unknown populations to.

Copy the RefInd files (bim, bed and fam) from your local data folder to the working folder:

Look at the .fam file, do you recognize some of these pops? There are two HapMap and three HGDP populations.

First, extract the SNPs we have in our data from the downloaded RefPops


```
plink --bfile refpops1 --extract MergeRef2.bim --make-bed --out refpops2  
```

Now we will merge our data with the downloaded data

```
plink --bfile MergeRef2 --bmerge refpops2.bed refpops2.bim refpops2.fam --make-bed --out MergeRefPop1  
```

Another strand issue, flip the strands of the refPop datasets to be merged


```
plink --bfile refpops2 --flip MergeRefPop1-merge.missnp --make-bed --out refpops3  
```

Try merge again:

```
plink --bfile MergeRef2 --bmerge refpops3.bed refpops3.bim refpops3.fam --make-bed --out MergeRefPop2 
```

It works now. Look at your screen output. You will see that the Refpops only contains SNPs that overlap with a small percentage of the SNPs in the UNknown Pops data (~15 000 vs ~95 000). We will now again filter for SNP missingness to exclude all of the extra SNPs in the Unknown Pop data (Retain only the overlap).

```
plink --bfile MergeRefPop2 --geno 0.1 --make-bed --out MergeRefPop2fil 
```

How many SNPs are left for your analyses?

The last thing to do is to extract your fake/Ref_ind from your data.

```
plink --bfile MergeRefPop2fil --remove RefInd1.fam --make-bed --out MergeRefPop3  
```

This is the final files for the next exercise. Rename them:

```
mv MergeRefPop3.bed PopStrucIn1.bed; mv MergeRefPop3.bim PopStrucIn1.bim; mv MergeRefPop3.fam PopStrucIn1.fam 
```

Now you have generated your input files for the next exercise which will deal with population structure analysis. You will look at the population structure of your unknown samples in comparison to the known reference populations from HapMap and HGDP.


The official Plink documentation (https://www.cog-genomics.org/plink/1.9/data) has a good explanation on how to all of the above.


## Step 8 Reflect and review

How many SNPs are left for your analyses? 
What was the genotyping rate for the combined reference dataset and what was it for the unknowns? 
Finally what is the genotyping rate for the final merged dataset? 

This are the final files for the next exercise. Now you have generated your input files for the next part which will deal with population structure analysis. You will look at the population structure of your unknown samples in comparison to the known reference populations that came from HapMap and HGDP.

=============================================================================

# PART 2: Population structure inference 

## Step 1 LD Pruning (OPTIONAL)

Before running PCA & ADMIXTURE it is advisable to prune the data to thin the marker set for linkage disequilibrium.
You can read up on how to prune for LD(https://dalexander.github.io/admixture/admixture-manual.pdf).

```plink --bfile YOUR_DATASET --indep-pairwise 10 10 0.1
   plink --bfile YOUR_DATASET --extract plink.prune.in --make-bed --out YOUR_DATASET_PRUNED
```

## Step 2 Principal component Analysis with Eigensoft

The first population structure method we will look at is Principal Components Analysis with Eigensoft. 

Look at the output PDF. How many of the PCs do you think contain useful information. What part of the variation is represented by each of the PCs. Can you see the percentage variation that each PC explains?

If we had a longer time to work on this exercize you would run a PCA analysis as explained below. Since we have one afternoon, it is good to look at the steps, understand them and maybe try them at home. After reading the steps, you can jump straight to the plotting step.

### Running the PCA analysis - we are skipping this step today, but feel free to try at home:

You need to run eigensoft on the .bed format from plink. You only need to modify your .fam file a little for it to work in Eigensoft. The .bed and .map files you use directly as-is. The .fam file you change the extension to .pedind and you substitute the last column (-9 at the moment indicating missing phenotype) with population numbers. When assigning pop numbers do not use 1, 2 or 9. They are reserved for cases, controls and missing data in Eigensoft.

Paste this piece of code in to the terminal:

```
cut -d " " -f1-5 PopStrucIn1.fam >file1a
cut -d " " -f1 PopStrucIn1.fam >file2a
sed "s/Unknown1/51/g" <file2a | sed "s/Unknown3/53/g" | sed "s/Unknown5/55/g" | sed "s/Unknown11/61/g" | sed "s/CEU/81/g" | sed "s/YRI/82/g" | sed "s/Han/83/g" | sed "s/San/84/g" | sed "s/MbutiPygmies/85/g" >file3a
paste file1a file3a >fileComb
sed "s/\t/ /g" fileComb > PopStrucIn1.pedind
rm file1a; rm file2a; rm file3a; rm fileComb
```
It will make a `.pedind` file from your `.fam` file.

Furthermore, you need a parameter file to indicate your parameter options to EIGENSOFT.

Copy the prepared parameter file from the `DATA` directory to your working folder it's called 
`PopStrucIn1.par`

Open the parameter file and look at what is specified in it. At the start is the input/output files. Furthermore, we ask for the info for 10 PCs to be output, qtmode is set to NO to indicate more than one pop, we prune the SNPs based on LD for an r2 value of 0.2. We don't remove any outlying points and we limit the sample size to 20. This is important for PCA where there are groups with very large sample sizes since large sample sizes will distort the PC plot. It is best for PCA that sample sizes are as even as possible.

Run the smartpca package in Eigensoft by typing

```
module load eigensoft

smartpca -p PopStrucIn1.par
```

The outputfiles are .evec and .eval

In the `.evec` file is the main output for the number of PCs that you specified in the .par file. The first row is the Eigenvalues for each of your PCs the rest of the rows list your Pop:Ind specification, the PCs and their loadings and your PopNumber at the end. in the `.eval` is all the eigenvalues that were extracted. To work out the percentage of variation each PC explains, you divide your particular PC eigenvalue by the sum over all the eigenvalues.

### Plotting the PCA in R (Doing this today)

Prep for R:

```
sed 1d PopStrucIn1.evec | sed  "s/:/   /g " >   PopStrucIn1.evecm
```

Load the R module:
```
module load R/3.4.3
```
See if you understand the code below. It plots PC1vsPC2 etc and puts labels on the plot.
Open `R` and paste the following code to plot your PCs:

```
WD<-getwd()
setwd(WD)
## Define these
evec<- read.table ("PopStrucIn1.evecm")
eval<- read.table ("PopStrucIn1.eval")
namer <- "PopStrucIn1"
nrpc<-10
## Script start
totalev <-sum(eval)
aa <- array(NA,dim=c(nrpc,1))
for (i in 1:nrpc) {
    aa[i,1]<-format(round(((eval[i,1]/totalev)*100),3), nsmall = 3)}
pdf (file =paste(namer, "_PCA1.pdf", sep=""), width =10, height = 15, pointsize =12)
par(mfrow=c(3,2), oma=c(0,0,4,0))
plot (evec[,3], evec[,4],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC1: ", aa[1,1], "%", sep=""), ylab=paste("PC2: ", aa[2,1], "%", sep=""))
legend("topright",  legend = unique(evec[,1]), text.col = "black", cex = 0.75, pch =unique(evec[,1]), col = unique(evec[,1]), xpd = TRUE, bty="n", )
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
plot (evec[,5], evec[,6],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC3: ", aa[3,1], "%", sep=""), ylab=paste("PC4: ", aa[4,1], "%", sep=""))
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
plot (evec[,7], evec[,8],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC5: ", aa[5,1], "%", sep=""), ylab=paste("PC6: ", aa[6,1], "%", sep=""))
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
plot (evec[,9], evec[,10],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC7: ", aa[7,1], "%", sep=""), ylab=paste("PC8: ", aa[8,1], "%", sep=""))
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
plot (evec[,11], evec[,12],  pch = as.numeric(evec[,1]), cex = 1, col = as.numeric(evec[,1]), xlab=paste("PC9: ", aa[9,1], "%", sep=""), ylab=paste("PC10: ", aa[10,1], "%", sep=""))
abline(h=0, col="lightgray", lty=5, lwd=0.8); abline(v=0, col="lightgray", lty=5, lwd=0.8)
barplot (as.numeric(aa[,1]), xlab = "PC", ylab = "%Variation explained", axes=TRUE)
title(paste(namer, "PC plot"), outer=TRUE, cex.main = 1)
dev.off()
q()
```
Once the plot is done, and it will be very quick you will be asked the following:

```
Save workspace image? [y/n/c]:
```
Type `y` and press enter. A PDF will have been generated and all you need to do is to copy the PDF file over to your computer (You can use the -scp command). In order to view it you need to use a X11 connection, but the datalabs should be equipped with it.

Look at the output PDF. Do the results of your population PCA correspond to the population structure results you got from the ADMIXTURE plots? How many of the PCs do you think contain useful information. What part of the variation is represented by each of the PCs. Can you see the percentage variation that each PC explains?

### Projected PCA (Skipping this today, again, feel free to try at home)

We will set-up a projected PCA run in Eigensoft so that the PCs are calculated based on the Ref pops only and the Unknown individuals are projected on these PCs based on the Ref pops.

To do that we need a slightly modified .par file and an additional file that lists the pops to use as ref pops. Copy these two files from the `SCRIPTS` folder and open them to see how they differ from the .par file used above:
`PopStrucIn1_Proj.par`
RefGroups.groups

Run Eigensoft by typing:

```
smartpca -p PopStrucIn1_Proj.par
```

Prepare the output files for R and run the R script as explained above. Remember the output filenames changed thus adapt the scripts used above accordingly (both the bash script line and the first part of the R script enclosed by the “”)

Look at the output PDF, what has changed? This should give you a further indication as to who your Unknown populations are.


Lastly, we will look at which SNPs contribute to which axes in the PC (SNP weightings of each principal component). This is one way to identify the most informative SNPs that define the structure between certain populations. This is useful if you want to look at population structure but you only want to pick a few best SNPs to type. To generate such a list you just add the snpweightoutname to your par file. We will also not prune for LD so that we do not exclude possible informative SNPs
 
Copy the modified .par file look how it looks and run Eigensoft
PopStrucIn1_snpweight.par

```
./smartpca -p PopStrucIn1_snpweight.par
```

Look at the `PopStrucIn1.snpweight_out` file. The file contains a list of snps and the weights they have on each PC. You can easily select and sort the list to obtain the SNPs with max info for a given PC.

Look at your previous generated PC plot `PopStrucIn1_Proj_PCA1.pdf`
Axis one (PC1) explains ~8% of the variation in your whole Ref_pop dataset. It is the axis that defines the difference between African and non-African populations. To generate a sorted list of the SNPs that would be the best to type to look at the difference between African and non-Africans paste the following command:

```
sed 's/ \+ /\t/g' PopStrucIn1.snpweight_out | sed 's/^\t//g' | cut -f1,3 | sort -r -n -k 3,3  >topSNPsPc1
```

Look at the topSNPsPc1 file that is generated


If you are interested in the frequency of the top SNP in your data. Copy the two scripts below:

```
Extract_snp_from_bed_make_Barplot
Extract_snp_from_bed_make_Barplot.R
```

Open the main script and paste the rs name of the SNP you are interested in in the place where rs00000000 is at the moment. Save the script and run by typing:

```
./Extract_snp_from_bed_make_Barplot
```

## Step 3 ADMIXTURE (Same as with the PCA, today we are skipping the analysis part, and going directly to the plotting - Step 4)

ADMIXTURE is a similar tool to STRUCTURE but runs much quicker, especially on large datasets.
ADMIXTURE runs directly from .bed or .ped files and needs no extra parameters for file preparation. You do not specify burin and repeats, ADMIXTURE exits when it converged on a solution (Delta< minimum value)

First, you have to load the module:

```
module load bioinfo-tools
module load ADMIXTURE/1.3.0
```
A basic ADMIXTURE run looks like this:

```
admixture -s time YOUR_PRUNED_DATASET.bed 2
```

This command executes the program with a seed set from system clock time, it gives the input file (remember the extension) and the K value at which to run ADMIXTURE (2 in the previous command).

For ADMIXTURE you also need to run many iterations at each K value, thus a compute cluster and some scripting is useful.

Make a script from the code below to run Admixture for K = 2-6 with 3 iterations at each K value.

```
#!/bin/bash
for kval in {2..6}; do
   for itir in {1..3}; do
           (echo '#!/bin/bash -l'
           echo "
           mkdir ${kval}.${itir}
           cd ${kval}.${itir}
#working folder where admixture and the bed files are
module load bioinfo-tools
module load ADMIXTURE/1.3.0
admixture -j3 -s $RANDOM path/to/your/YOUR_PRUNED_DATASET.bed ${kval}
cd ../
rm -r ${kval}.${int}
#moves it to a different folder and renames it so you end up with multiple iterations
exit 0") |
               sbatch -M snowy -p core -n 3 -t 72:0:0 -A YOUR_UPPMAX_PROJECT_HERE -J ADMX.${kval}.${itir} -o ADMX.${kval}.${itir}.output -e ADMX${kval}.${itir}.output
       done
done
```

You will need to **replace the project name** and the **path** to your pruned .bed-file in the script then just run it:

```
bash NAME_OF_THE_SCRIPT.sh
```
After a successful Admixture run, you should be seeing a bunch of new folders being created. 
In each one of them you will find the output ```P``` and ```Q``` for each iteration. 

*If you decide to work on PONG locally, you need to edit each ```Q``` file by adding the right k & iteration number to each one. After doing this for all folders, you can move all the ```Q``` files to a directory of your liking.*

## Step 4 PONG 

When the admixture analysis is finally done we use PONG to combine the different iterations and visualize our results. 

To be able to run PONG we thus need to generate **three different files**.

The first being the ***filemap***. This is the only input that is strictly required to run PONG. It consists of three columns.
From the PONG manual: 


```
Column 1. The runID, a unique label for the Q matrix (e.g. the string “run5_K7”).

Column 2. The K value for the Q matrix. Each value of K between Kmin and Kmax must
be represented by at least one Q matrix in the filemap; if not, pong will abort.

Column 3. The path to the Q matrix, relative to the location of the filemap. 
```

In order to create what we need we can edit and run the loop below. Make sure to edit the right number of **k** and **i**.
Also check whether the prefix of the admixture output Q files is the same in both the loop and the Q files. As long as you are starting PONG in the same directory of all the Admixture output files you should be ok.

```
for i in {2..10};
do
    for j in {1..15};
    do
    echo -e "k${i}_r${j}\t${i}\t${i}.${j}/YOUR_DATASET.${i}.Q" >> unknown_FILEMAP.txt
    done
done

```
Example of what this file looks like, depending on how many K and how many iterations you run:

```
k2_r1	2	2.1/YOUR_DATASET.2.Q
k2_r2	2	2.2/YOUR_DATASET.2.Q
k2_r3	2	2.3/YOUR_DATASET.2.Q
k3_r1	3	3.1/YOUR_DATASET.3.Q
k3_r2	3	3.2/YOUR_DATASET.3.Q
k3_r3	3	3.3/YOUR_DATASET.3.Q
k4_r1	4	4.1/YOUR_DATASET.4.Q
k4_r2	4	4.2/YOUR_DATASET.4.Q
k4_r3	4	4.3/YOUR_DATASET.4.Q
k5_r1	5	5.1/YOUR_DATASET.5.Q
k5_r2	5	5.2/YOUR_DATASET.5.Q
k5_r3	5	5.3/YOUR_DATASET.5.Q
k6_r1	6	6.1/YOUR_DATASET.6.Q
k6_r2	6	6.2/YOUR_DATASET.6.Q
k6_r3	6	6.3/YOUR_DATASET.6.Q
k7_r1	7	7.1/YOUR_DATASET.7.Q
k7_r2	7	7.2/YOUR_DATASET.7.Q
k7_r3	7	7.3/YOUR_DATASET.7.Q
```

The next file we need to create is the ***ind2pop*** file. It is just a list of which population each individual belongs to.
We have this information in the `.fam` file so we can just cut out the field we need:

```
cut -f 1 -d " " YOUR_DATASET.fam > unknown_IND2POP.txt

``` 
The ind2pop file should look exactly like the first column of the fam.

The ***poporder*** file is a key between what your populations are called and what "Proper" name you want to show up in your final plot. 
Also as the name suggests, **gives the order in which you want the populations to be in**, so order them as you think it makes most sense.

*Note that the file needs to be **tab-delimited**, i.e separated by tabs (If you have spaces issues may occure). 
Also, make sure within the names in this file to not have any spaces. If you do, try replacing them with underscores (_).*
*Again, be careful when editing this file, typos, extra spaces etc might cause problems.*

Below is just an example of what a poporder file should look like:

```
aDNA_Rick	pickle
aDNA_Morty	m0rty
aDNA_Ragnar	rgnr
aDNA_martians	mrt99
Napoleon_Bonaparte	nb001
Netherlands_BA	Dutch
```
An easy way of doing this is by :
```
cut -f 1 -d " " YOUR_DATASET.fam | uniq > unknown_POPORDER_prep.txt
```
and then just use this simple script:
```
#!usr/bin/env bash

file=$(cat unknown_POPORDER_prep.txt)

for line in $file
do
    echo -e "$line\t$line" >> unknown_POPORDER.txt
done
```
This way you will get a file with two columns. You can edit the the names of the populations you are interested in by changing the names of the second column.

### Now we have all the files we need. Time to run PONG.

*It is by far best to run PONG on your local computer. All you need to do is install PONG and copy the ```.Q``` files to your computer together with the three files (filemap,ind2pop,poporder) and run it. If you don't know how to install PONG(https://github.com/ramachandran-lab/pong), it is available through the module system on Uppmax, although very often it is quite laggy. 

* Mac Users - you will need to install XQuartz (an open-source version of the X.Org X server, a component of the X Window System that runs on macOS)https://www.xquartz.org
* Windows Users - MobaXterm https://mobaxterm.mobatek.net
* Ubuntu Users - I'm sure you can figure it out yourselves

```
module load pong 
```
**Super important:**
Since we are several people who are going to run PONG at the same time we need to use a different port, otherwise, we will collide with each other. The default port for PONG is 4000. Any other free port will work, like 4001, 2, etc. Make sure you are using a **unique port** before proceeding. If multiple people are trying to run PONG on the same port you have problems, so talk to your classmates and find a unique port in the 4000s that people aren't using.

```
pong -m unknown_FILEMAP.txt -i unknown_IND2POP.txt -n unknown_POPORDER.txt -g --port YOUR_PORT_NUMBER_HERE
```

When PONG is done it will start hosting a webserver that displays the results at port 4000 by default:  http://localhost:4000. Pong needs to be running for you to look at the results, i.e. **if you close it it will not work.**
It will look like this:

```
-------------------------------------------------------------------
                            p o n g
      by A. Behr, K. Liu, T. Devlin, G. Liu-Fang, and S. Ramachandran
                       Version 1.5 (2021)
-------------------------------------------------------------------
-------------------------------------------------------------------

Parsing input and generating cluster network graph
Matching clusters within each K and finding representative runs
For K=2, there are 3 modes across 3 runs.
For K=3, there are 3 modes across 3 runs.
For K=4, there are 3 modes across 3 runs.
For K=5, there are 3 modes across 3 runs.
For K=6, there are 3 modes across 3 runs.
For K=7, there are 3 modes across 3 runs.
Matching clusters across K
Finding best alignment for all runs within and across K
match time: 0.64s
align time: 0.01s
total time: 1.21s
-----------------------------------------------------------
pong server is now running locally & listening on port 4005
Open your web browser and navigate to http://localhost:4005 to see the visualization
```

The web server is hosted on the **same login node as you were running pong**. In case you are unsure of which one that is you can use the command `hostname` to figure it out:

```
hostname
```

To view files interactively you need to have an X11 connection. So when you connect to rackham from a new tab do:

```
ssh -AY YOUR_USERNAME_HERE@rackham.uppmax.uu.se
```

Make sure that you connect to **the same Rackham (i.e 1,2,3 etc) as you got from hostname**.  
In a **new tab** (if you didn't put PONG in the background) type:

```
firefox http://localhost:YOUR_PORT_NUMBER
```


Once you are finished with looking at the PONG output you can click and save some output to file and then close the program by hitting `ctrl - c` in the terminal tab running it. 

### What do the results from Admixture & PCA tell you? 
What do you see? What information does this give you about your mysterious unknown populations? 
Do the results of your PCA correspond to the population structure results you got from the ADMIXTURE plots? Can you figure out who they are?

**After you have finished with your analyses, delete all unnecessary files from your folder.**

# Part 3: Review & reflect

Okay, hopefully this project wasn't too difficult or too boring or too fast. Maybe it was even fun? And aside from that maybe you even learnt something. Regardless of how you felt about it, this project was constructed to provide you with a **real life** scenario (in the condensed time we have) of what most exploratory analysis our field of study look like, warts & all. More times than not you are working with something you don't know enough about (duh! - that's why it's called research) using techniques you're trying for the first time and failing on many attempts. Most of the manuals & how-toos you need to look for yourself, and references come in the shape of many scattered papers on the subject. >>>>>>>>>>>>>>>>>>>>And as you would do when you want to publish your findings in real life, the final step of the project is to write a 10-15 page report on your findings. Make sure to read up on the papers recommended above. They should provide you with enough of a backstory & will surely help you a lot in constructing the grand picture of things. You are free to also look into similar papers on the subject. Good luck & happy writing!
 
